{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx7xddgVzHR1"
      },
      "source": [
        "# Generate Hoax News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2afE9jZ2zN8f"
      },
      "source": [
        "## Member of Group\n",
        "- Elisa Bayu Hendra\n",
        "- Yohanes Emmanuel Putra Sutanto\n",
        "- Federico Matthew Pratama\n",
        "- Fernando Perry\n",
        "- Vincentius Johanes Lwie Jaya\n",
        "- Yohanes Bramanta Adita Saputra"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Versi 1 (The Best Choice)"
      ],
      "metadata": {
        "id": "aTZIi4Ic3I73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation / Setup (Versi 1)"
      ],
      "metadata": {
        "id": "L3hFxSrp426v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Library"
      ],
      "metadata": {
        "id": "Zid8Mmjf45_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.cluster import KMeans\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
        "import random\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "GJl5zQMp4435"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Code Generate Hoax (Versi 1 - Basic)"
      ],
      "metadata": {
        "id": "x7Qad1ZMV9kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsChainGenerator:\n",
        "    def __init__(self, use_gpu=True):\n",
        "        \"\"\"\n",
        "        Initialize the news generator with AMD GPU support\n",
        "        \"\"\"\n",
        "        # Check for GPU availability (works with AMD ROCm and NVIDIA CUDA)\n",
        "        if use_gpu and torch.cuda.is_available():\n",
        "            self.device = \"cuda\"\n",
        "            print(f\"Using GPU: {torch.cuda.get_device_name()}\")\n",
        "        elif use_gpu and hasattr(torch, 'xpu') and torch.xpu.is_available():\n",
        "            self.device = \"xpu\"  # Intel GPU support\n",
        "            print(\"Using Intel XPU\")\n",
        "        else:\n",
        "            self.device = \"cpu\"\n",
        "            print(\"Using CPU\")\n",
        "\n",
        "        # Load 20 newsgroups data\n",
        "        print(\"Loading 20 Newsgroups dataset...\")\n",
        "        self.newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "        self.categories = self.newsgroups.target_names\n",
        "\n",
        "        # Initialize components\n",
        "        self.vectorizer = TfidfVectorizer(max_features=5000, stop_words='english',\n",
        "                                        min_df=2, max_df=0.8)\n",
        "        self.topic_model = LatentDirichletAllocation(n_components=20, random_state=42)\n",
        "\n",
        "        # Markov chains for each category\n",
        "        self.markov_chains = {}\n",
        "        self.topic_keywords = {}\n",
        "\n",
        "        # Initialize lightweight language model\n",
        "        self.init_language_model()\n",
        "\n",
        "    def init_language_model(self):\n",
        "        \"\"\"Initialize a lightweight language model for coherent text generation\"\"\"\n",
        "        try:\n",
        "            # Use DistilGPT2 for faster processing\n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            # Initialize text generation pipeline with GPU support\n",
        "            self.text_generator = pipeline(\n",
        "                'text-generation',\n",
        "                model='distilgpt2',\n",
        "                tokenizer=self.tokenizer,\n",
        "                device=0 if self.device == \"cuda\" else -1,\n",
        "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "            )\n",
        "            print(\"Language model initialized successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing language model: {e}\")\n",
        "            self.text_generator = None\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Clean and preprocess text\"\"\"\n",
        "        # Remove email addresses, URLs, and special characters\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
        "        return text\n",
        "\n",
        "    def build_markov_chains(self, order=2):\n",
        "        \"\"\"Build Markov chains for each news category\"\"\"\n",
        "        print(\"Building Markov chains for each category...\")\n",
        "\n",
        "        for i, category in enumerate(self.categories):\n",
        "            # Get texts for this category\n",
        "            category_texts = [self.newsgroups.data[j] for j, target in enumerate(self.newsgroups.target) if target == i]\n",
        "\n",
        "            # Combine and preprocess texts\n",
        "            combined_text = ' '.join([self.preprocess_text(text) for text in category_texts[:100]])  # Limit for speed\n",
        "            words = combined_text.split()\n",
        "\n",
        "            if len(words) < order + 1:\n",
        "                continue\n",
        "\n",
        "            # Build Markov chain\n",
        "            chain = defaultdict(list)\n",
        "            for j in range(len(words) - order):\n",
        "                key = tuple(words[j:j + order])\n",
        "                next_word = words[j + order]\n",
        "                chain[key].append(next_word)\n",
        "\n",
        "            self.markov_chains[category] = dict(chain)\n",
        "            print(f\"Built chain for {category}: {len(chain)} states\")\n",
        "\n",
        "    def extract_topic_keywords(self):\n",
        "        \"\"\"Extract key topics and keywords using LDA\"\"\"\n",
        "        print(\"Extracting topic keywords...\")\n",
        "\n",
        "        # Vectorize texts\n",
        "        tfidf_matrix = self.vectorizer.fit_transform([self.preprocess_text(text) for text in self.newsgroups.data])\n",
        "\n",
        "        # Fit topic model\n",
        "        self.topic_model.fit(tfidf_matrix)\n",
        "\n",
        "        # Extract keywords for each topic\n",
        "        feature_names = self.vectorizer.get_feature_names_out()\n",
        "\n",
        "        for topic_idx, topic in enumerate(self.topic_model.components_):\n",
        "            top_keywords = [feature_names[i] for i in topic.argsort()[-10:]]\n",
        "            self.topic_keywords[self.categories[topic_idx % len(self.categories)]] = top_keywords\n",
        "\n",
        "        print(\"Topic extraction completed\")\n",
        "\n",
        "    def generate_with_markov(self, category, length=50):\n",
        "        \"\"\"Generate text using Markov chain\"\"\"\n",
        "        if category not in self.markov_chains:\n",
        "            return \"\"\n",
        "\n",
        "        chain = self.markov_chains[category]\n",
        "        if not chain:\n",
        "            return \"\"\n",
        "\n",
        "        # Start with random key\n",
        "        current_key = random.choice(list(chain.keys()))\n",
        "        result = list(current_key)\n",
        "\n",
        "        for _ in range(length - len(current_key)):\n",
        "            if current_key not in chain:\n",
        "                break\n",
        "            next_word = random.choice(chain[current_key])\n",
        "            result.append(next_word)\n",
        "            current_key = tuple(result[-2:])  # For order=2\n",
        "\n",
        "        return ' '.join(result)\n",
        "\n",
        "    def enhance_with_llm(self, seed_text, category, max_length=200):\n",
        "        \"\"\"Enhance text using language model for better coherence\"\"\"\n",
        "        if not self.text_generator:\n",
        "            return seed_text\n",
        "\n",
        "        try:\n",
        "            # Add category context\n",
        "            prompt = f\"News about {category.replace('_', ' ')}: {seed_text}\"\n",
        "\n",
        "            # Generate enhanced text\n",
        "            generated = self.text_generator(\n",
        "                prompt,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.8,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "            return generated[0]['generated_text'].replace(prompt, '').strip()\n",
        "        except Exception as e:\n",
        "            print(f\"LLM enhancement failed: {e}\")\n",
        "            return seed_text\n",
        "\n",
        "    def generate_structured_article(self, category, target_length=300):\n",
        "        \"\"\"Generate a structured news article\"\"\"\n",
        "        # Generate different parts using Markov chains\n",
        "        headline = self.generate_with_markov(category, length=8)\n",
        "        intro = self.generate_with_markov(category, length=30)\n",
        "        body1 = self.generate_with_markov(category, length=40)\n",
        "        body2 = self.generate_with_markov(category, length=40)\n",
        "        conclusion = self.generate_with_markov(category, length=20)\n",
        "\n",
        "        # Combine parts\n",
        "        base_article = f\"{headline}. {intro} {body1} {body2} {conclusion}\"\n",
        "\n",
        "        # Enhance with LLM if available\n",
        "        if self.text_generator:\n",
        "            enhanced_article = self.enhance_with_llm(base_article, category, target_length)\n",
        "            return enhanced_article\n",
        "\n",
        "        return base_article\n",
        "\n",
        "    def generate_hoax_news(self, num_articles=5, categories=None):\n",
        "        \"\"\"Generate multiple hoax news articles\"\"\"\n",
        "        if categories is None:\n",
        "            categories = random.sample(self.categories, min(5, len(self.categories)))\n",
        "\n",
        "        print(f\"Generating {num_articles} articles across {len(categories)} categories...\")\n",
        "\n",
        "        articles = []\n",
        "        for i in range(num_articles):\n",
        "            category = random.choice(categories)\n",
        "\n",
        "            print(f\"Generating article {i+1}/{num_articles} for category: {category}\")\n",
        "\n",
        "            article = {\n",
        "                'id': i + 1,\n",
        "                'category': category,\n",
        "                'title': self.generate_with_markov(category, length=6).title(),\n",
        "                'content': self.generate_structured_article(category),\n",
        "                'keywords': self.topic_keywords.get(category, [])[:5]\n",
        "            }\n",
        "\n",
        "            articles.append(article)\n",
        "\n",
        "        return articles\n",
        "\n",
        "    def train_and_generate(self, num_articles=5):\n",
        "        \"\"\"Main method to train models and generate articles\"\"\"\n",
        "        print(\"Starting hoax news generation for research purposes...\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Build models\n",
        "        self.build_markov_chains()\n",
        "        self.extract_topic_keywords()\n",
        "\n",
        "        # Generate articles\n",
        "        articles = self.generate_hoax_news(num_articles)\n",
        "\n",
        "        return articles\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"Synthetic News Generator for Research\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"WARNING: This tool generates synthetic content for research purposes only!\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Initialize generator\n",
        "    generator = NewsChainGenerator(use_gpu=True)\n",
        "\n",
        "    # Generate articles\n",
        "    articles = generator.train_and_generate(num_articles=3)  # Start with 3 for testing\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"GENERATED ARTICLES (FOR RESEARCH ONLY)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for article in articles:\n",
        "        print(f\"\\nArticle {article['id']}\")\n",
        "        print(f\"Category: {article['category']}\")\n",
        "        print(f\"Title: {article['title']}\")\n",
        "        print(f\"Keywords: {', '.join(article['keywords'])}\")\n",
        "        print(\"-\" * 40)\n",
        "        print(f\"Content: {article['content'][:300]}...\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    # Save to file\n",
        "    df = pd.DataFrame(articles)\n",
        "    df.to_csv('synthetic_news_research.csv', index=False)\n",
        "    print(f\"\\nResults saved to 'synthetic_news_research.csv'\")\n",
        "\n",
        "    return articles\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds for reproducibility\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    articles = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f3p9Hb-WQgd",
        "outputId": "20bad8ab-957e-4ced-dcb0-b2389d65ff37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic News Generator for Research\n",
            "========================================\n",
            "WARNING: This tool generates synthetic content for research purposes only!\n",
            "========================================\n",
            "Using CPU\n",
            "Loading 20 Newsgroups dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language model initialized successfully\n",
            "Starting hoax news generation for research purposes...\n",
            "============================================================\n",
            "Building Markov chains for each category...\n",
            "Built chain for alt.atheism: 11899 states\n",
            "Built chain for comp.graphics: 7868 states\n",
            "Built chain for comp.os.ms-windows.misc: 7805 states\n",
            "Built chain for comp.sys.ibm.pc.hardware: 8946 states\n",
            "Built chain for comp.sys.mac.hardware: 6537 states\n",
            "Built chain for comp.windows.x: 10717 states\n",
            "Built chain for misc.forsale: 7354 states\n",
            "Built chain for rec.autos: 9229 states\n",
            "Built chain for rec.motorcycles: 8505 states\n",
            "Built chain for rec.sport.baseball: 10053 states\n",
            "Built chain for rec.sport.hockey: 9277 states\n",
            "Built chain for sci.crypt: 17324 states\n",
            "Built chain for sci.electronics: 9710 states\n",
            "Built chain for sci.med: 10469 states\n",
            "Built chain for sci.space: 15382 states\n",
            "Built chain for soc.religion.christian: 16288 states\n",
            "Built chain for talk.politics.guns: 13218 states\n",
            "Built chain for talk.politics.mideast: 20442 states\n",
            "Built chain for talk.politics.misc: 20884 states\n",
            "Built chain for talk.religion.misc: 13747 states\n",
            "Extracting topic keywords...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic extraction completed\n",
            "Generating 3 articles across 5 categories...\n",
            "Generating article 1/3 for category: alt.atheism\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating article 2/3 for category: alt.atheism\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating article 3/3 for category: talk.politics.guns\n",
            "\n",
            "============================================================\n",
            "GENERATED ARTICLES (FOR RESEARCH ONLY)\n",
            "============================================================\n",
            "\n",
            "Article 1\n",
            "Category: alt.atheism\n",
            "Title: Said Unto Them Ye Do Err\n",
            "Keywords: file, using, email, program, know\n",
            "----------------------------------------\n",
            "Content: that what i am really being warned about is a religion in fact a religion with a vast majority of those who are atheists and for the purpose of promoting a religious liberty agenda are the ones who have also been the real world based on non-believers.\n",
            "I think that if i were to say that this is a rel...\n",
            "------------------------------------------------------------\n",
            "\n",
            "Article 2\n",
            "Category: alt.atheism\n",
            "Title: The Lack Of Scepticism And So\n",
            "Keywords: file, using, email, program, know\n",
            "----------------------------------------\n",
            "Content: of a fomenting state of barbarism and the human character's will to fight back against the evil of being a man. This is the same attitude as saying that the tl the nd a s is a sin, but that the nd a s is a sin, but that the nd a s is a sin. Therefore, if there is no other way of explaining this, the...\n",
            "------------------------------------------------------------\n",
            "\n",
            "Article 3\n",
            "Category: talk.politics.guns\n",
            "Title: Have Immediate Access To At The\n",
            "Keywords: thanks, mouse, hard, mb, controller\n",
            "----------------------------------------\n",
            "Content: are to be sure they can also keep more guns in the house which is what they are making fun of people who do have access to guns and it gets more like this gun gets the most out of them.\n",
            "\n",
            "\n",
            "A good example of the gun control of \"Gun Owners of America\" is the gun control of congressperson Tom Brady. He ...\n",
            "------------------------------------------------------------\n",
            "\n",
            "Results saved to 'synthetic_news_research.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation per Function"
      ],
      "metadata": {
        "id": "Z6wkwFF7LZc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialization Phase"
      ],
      "metadata": {
        "id": "7NaHC8JIRvkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsChainGenerator:\n",
        "    def __init__(self, use_gpu=True):\n",
        "        \"\"\"\n",
        "        Initialize the news generator with AMD GPU support\n",
        "        \"\"\"\n",
        "        # Check for GPU availability (works with AMD ROCm and NVIDIA CUDA)\n",
        "        if use_gpu and torch.cuda.is_available():\n",
        "            self.device = \"cuda\"\n",
        "            print(f\"Using GPU: {torch.cuda.get_device_name()}\")\n",
        "        elif use_gpu and hasattr(torch, 'xpu') and torch.xpu.is_available():\n",
        "            self.device = \"xpu\"  # Intel GPU support\n",
        "            print(\"Using Intel XPU\")\n",
        "        else:\n",
        "            self.device = \"cpu\"\n",
        "            print(\"Using CPU\")\n",
        "\n",
        "        # Load 20 newsgroups data\n",
        "        print(\"Loading 20 Newsgroups dataset...\")\n",
        "        self.newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "        self.categories = self.newsgroups.target_names\n",
        "\n",
        "        # Initialize components\n",
        "        self.vectorizer = TfidfVectorizer(max_features=5000, stop_words='english',\n",
        "                                        min_df=2, max_df=0.8)\n",
        "        self.topic_model = LatentDirichletAllocation(n_components=20, random_state=42)\n",
        "\n",
        "        # Markov chains for each category\n",
        "        self.markov_chains = {}\n",
        "        self.topic_keywords = {}\n",
        "\n",
        "        # Initialize lightweight language model\n",
        "        self.init_language_model()"
      ],
      "metadata": {
        "id": "VhNofjkaR0LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Language Model Preparation"
      ],
      "metadata": {
        "id": "LrgOa6mRZpi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_language_model(self):\n",
        "        \"\"\"Initialize a lightweight language model for coherent text generation\"\"\"\n",
        "        try:\n",
        "            # Use DistilGPT2 for faster processing\n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            # Initialize text generation pipeline with GPU support\n",
        "            self.text_generator = pipeline(\n",
        "                'text-generation',\n",
        "                model='distilgpt2',\n",
        "                tokenizer=self.tokenizer,\n",
        "                device=0 if self.device == \"cuda\" else -1,\n",
        "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "            )\n",
        "            print(\"Language model initialized successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing language model: {e}\")\n",
        "            self.text_generator = None"
      ],
      "metadata": {
        "id": "4HvpPhtqZydk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Basic Text Cleaning & Normalization"
      ],
      "metadata": {
        "id": "wBdU7opkZ4We"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(self, text):\n",
        "        \"\"\"Clean and preprocess text\"\"\"\n",
        "        # Remove email addresses, URLs, and special characters\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
        "        return text"
      ],
      "metadata": {
        "id": "R41tidZLZ69K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Markov Chain Model Training"
      ],
      "metadata": {
        "id": "bRNPKPiCaCcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_markov_chains(self, order=2):\n",
        "        \"\"\"Build Markov chains for each news category\"\"\"\n",
        "        print(\"Building Markov chains for each category...\")\n",
        "\n",
        "        for i, category in enumerate(self.categories):\n",
        "            # Get texts for this category\n",
        "            category_texts = [self.newsgroups.data[j] for j, target in enumerate(self.newsgroups.target) if target == i]\n",
        "\n",
        "            # Combine and preprocess texts\n",
        "            combined_text = ' '.join([self.preprocess_text(text) for text in category_texts[:100]])  # Limit for speed\n",
        "            words = combined_text.split()\n",
        "\n",
        "            if len(words) < order + 1:\n",
        "                continue\n",
        "\n",
        "            # Build Markov chain\n",
        "            chain = defaultdict(list)\n",
        "            for j in range(len(words) - order):\n",
        "                key = tuple(words[j:j + order])\n",
        "                next_word = words[j + order]\n",
        "                chain[key].append(next_word)\n",
        "\n",
        "            self.markov_chains[category] = dict(chain)\n",
        "            print(f\"Built chain for {category}: {len(chain)} states\")"
      ],
      "metadata": {
        "id": "b-4TUjF-aHxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Topic Modeling & Keyword Extraction"
      ],
      "metadata": {
        "id": "B1M9s8C_aNnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_topic_keywords(self):\n",
        "        \"\"\"Extract key topics and keywords using LDA\"\"\"\n",
        "        print(\"Extracting topic keywords...\")\n",
        "\n",
        "        # Vectorize texts\n",
        "        tfidf_matrix = self.vectorizer.fit_transform([self.preprocess_text(text) for text in self.newsgroups.data])\n",
        "\n",
        "        # Fit topic model\n",
        "        self.topic_model.fit(tfidf_matrix)\n",
        "\n",
        "        # Extract keywords for each topic\n",
        "        feature_names = self.vectorizer.get_feature_names_out()\n",
        "\n",
        "        for topic_idx, topic in enumerate(self.topic_model.components_):\n",
        "            top_keywords = [feature_names[i] for i in topic.argsort()[-10:]]\n",
        "            self.topic_keywords[self.categories[topic_idx % len(self.categories)]] = top_keywords"
      ],
      "metadata": {
        "id": "uEV62ojvaQOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Basic Markov Chain Text Generation"
      ],
      "metadata": {
        "id": "5pyqj3ZPaZ7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_markov(self, category, length=50):\n",
        "        \"\"\"Generate text using Markov chain\"\"\"\n",
        "        if category not in self.markov_chains:\n",
        "            return \"\"\n",
        "\n",
        "        chain = self.markov_chains[category]\n",
        "        if not chain:\n",
        "            return \"\"\n",
        "\n",
        "        # Start with random key\n",
        "        current_key = random.choice(list(chain.keys()))\n",
        "        result = list(current_key)\n",
        "\n",
        "        for _ in range(length - len(current_key)):\n",
        "            if current_key not in chain:\n",
        "                break\n",
        "            next_word = random.choice(chain[current_key])\n",
        "            result.append(next_word)\n",
        "            current_key = tuple(result[-2:])  # For order=2\n",
        "\n",
        "        return ' '.join(result)"
      ],
      "metadata": {
        "id": "DDx1iTwGajT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LLM-Based Text Enhancement"
      ],
      "metadata": {
        "id": "1t1FDNBvaobf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def enhance_with_llm(self, seed_text, category, max_length=200):\n",
        "        \"\"\"Enhance text using language model for better coherence\"\"\"\n",
        "        if not self.text_generator:\n",
        "            return seed_text\n",
        "\n",
        "        try:\n",
        "            # Add category context\n",
        "            prompt = f\"News about {category.replace('_', ' ')}: {seed_text}\"\n",
        "\n",
        "            # Generate enhanced text\n",
        "            generated = self.text_generator(\n",
        "                prompt,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.8,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "            return generated[0]['generated_text'].replace(prompt, '').strip()\n",
        "        except Exception as e:\n",
        "            print(f\"LLM enhancement failed: {e}\")\n",
        "            return seed_text"
      ],
      "metadata": {
        "id": "ScRljWC9apRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Structured Article Assembly"
      ],
      "metadata": {
        "id": "09emCO-jatqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_structured_article(self, category, target_length=300):\n",
        "        \"\"\"Generate a structured news article\"\"\"\n",
        "        # Generate different parts using Markov chains\n",
        "        headline = self.generate_with_markov(category, length=8)\n",
        "        intro = self.generate_with_markov(category, length=30)\n",
        "        body1 = self.generate_with_markov(category, length=40)\n",
        "        body2 = self.generate_with_markov(category, length=40)\n",
        "        conclusion = self.generate_with_markov(category, length=20)\n",
        "\n",
        "        # Combine parts\n",
        "        base_article = f\"{headline}. {intro} {body1} {body2} {conclusion}\"\n",
        "\n",
        "        # Enhance with LLM if available\n",
        "        if self.text_generator:\n",
        "            enhanced_article = self.enhance_with_llm(base_article, category, target_length)\n",
        "            return enhanced_article\n",
        "\n",
        "        return base_article"
      ],
      "metadata": {
        "id": "w4OBlXUKavMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multi-Article Batch Generation"
      ],
      "metadata": {
        "id": "66-ongRAa1cQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_hoax_news(self, num_articles=5, categories=None):\n",
        "        \"\"\"Generate multiple hoax news articles\"\"\"\n",
        "        if categories is None:\n",
        "            categories = random.sample(self.categories, min(5, len(self.categories)))\n",
        "\n",
        "        print(f\"Generating {num_articles} articles across {len(categories)} categories...\")\n",
        "\n",
        "        articles = []\n",
        "        for i in range(num_articles):\n",
        "            category = random.choice(categories)\n",
        "\n",
        "            print(f\"Generating article {i+1}/{num_articles} for category: {category}\")\n",
        "\n",
        "            article = {\n",
        "                'id': i + 1,\n",
        "                'category': category,\n",
        "                'title': self.generate_with_markov(category, length=6).title(),\n",
        "                'content': self.generate_structured_article(category),\n",
        "                'keywords': self.topic_keywords.get(category, [])[:5]\n",
        "            }\n",
        "\n",
        "            articles.append(article)\n",
        "\n",
        "        return articles"
      ],
      "metadata": {
        "id": "KiOPHaEUa354"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Complete Pipeline Orchestration"
      ],
      "metadata": {
        "id": "ice3Eb43a9g0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_generate(self, num_articles=5):\n",
        "        \"\"\"Main method to train models and generate articles\"\"\"\n",
        "        print(\"Starting hoax news generation for research purposes...\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Build models\n",
        "        self.build_markov_chains()\n",
        "        self.extract_topic_keywords()\n",
        "\n",
        "        # Generate articles\n",
        "        articles = self.generate_hoax_news(num_articles)\n",
        "\n",
        "        return articles"
      ],
      "metadata": {
        "id": "qmVBqkNPa_mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main"
      ],
      "metadata": {
        "id": "lMraaSrObF9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"Synthetic News Generator for Research\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"WARNING: This tool generates synthetic content for research purposes only!\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Initialize generator\n",
        "    generator = NewsChainGenerator(use_gpu=True)\n",
        "\n",
        "    # Generate articles\n",
        "    articles = generator.train_and_generate(num_articles=3)  # Start with 3 for testing\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"GENERATED ARTICLES (FOR RESEARCH ONLY)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for article in articles:\n",
        "        print(f\"\\nArticle {article['id']}\")\n",
        "        print(f\"Category: {article['category']}\")\n",
        "        print(f\"Title: {article['title']}\")\n",
        "        print(f\"Keywords: {', '.join(article['keywords'])}\")\n",
        "        print(\"-\" * 40)\n",
        "        print(f\"Content: {article['content'][:300]}...\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    # Save to file\n",
        "    df = pd.DataFrame(articles)\n",
        "    df.to_csv('synthetic_news_research.csv', index=False)\n",
        "    print(f\"\\nResults saved to 'synthetic_news_research.csv'\")\n",
        "\n",
        "    return articles\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds for reproducibility\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    articles = main()"
      ],
      "metadata": {
        "id": "MWh9v23pbJKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Code Generate Hoax (Versi 1 - Advanced)"
      ],
      "metadata": {
        "id": "glbMxMRKWE_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedNewsChainGenerator:\n",
        "    def __init__(self, use_gpu=True, min_word_count=150):\n",
        "        \"\"\"\n",
        "        Initialize the enhanced news generator with better filtering\n",
        "        \"\"\"\n",
        "        # Check for GPU availability\n",
        "        if use_gpu and torch.cuda.is_available():\n",
        "            self.device = \"cuda\"\n",
        "            print(f\"Using GPU: {torch.cuda.get_device_name()}\")\n",
        "        elif use_gpu and hasattr(torch, 'xpu') and torch.xpu.is_available():\n",
        "            self.device = \"xpu\"\n",
        "            print(\"Using Intel XPU\")\n",
        "        else:\n",
        "            self.device = \"cpu\"\n",
        "            print(\"Using CPU\")\n",
        "\n",
        "        self.min_word_count = min_word_count\n",
        "        print(f\"Minimum word count per article: {min_word_count}\")\n",
        "\n",
        "        # Load 20 newsgroups data\n",
        "        print(\"Loading 20 Newsgroups dataset...\")\n",
        "        self.newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "        self.categories = self.newsgroups.target_names\n",
        "\n",
        "        # Initialize components\n",
        "        self.vectorizer = TfidfVectorizer(max_features=5000, stop_words='english',\n",
        "                                        min_df=2, max_df=0.8)\n",
        "        self.topic_model = LatentDirichletAllocation(n_components=20, random_state=42)\n",
        "\n",
        "        # Enhanced Markov chains\n",
        "        self.markov_chains = {}\n",
        "        self.word_frequencies = {}\n",
        "        self.sentence_starters = {}\n",
        "        self.sentence_enders = {}\n",
        "        self.topic_keywords = {}\n",
        "\n",
        "        # Common English words for validation\n",
        "        self.common_words = set([\n",
        "            'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'it', 'for', 'not', 'on',\n",
        "            'with', 'he', 'as', 'you', 'do', 'at', 'this', 'but', 'his', 'by', 'from', 'they',\n",
        "            'we', 'say', 'her', 'she', 'or', 'an', 'will', 'my', 'one', 'all', 'would', 'there',\n",
        "            'their', 'what', 'so', 'up', 'out', 'if', 'about', 'who', 'get', 'which', 'go', 'me',\n",
        "            'when', 'make', 'can', 'like', 'time', 'no', 'just', 'him', 'know', 'take', 'people',\n",
        "            'into', 'year', 'your', 'good', 'some', 'could', 'them', 'see', 'other', 'than', 'then',\n",
        "            'now', 'look', 'only', 'come', 'its', 'over', 'think', 'also', 'back', 'after', 'use',\n",
        "            'two', 'how', 'our', 'work', 'first', 'well', 'way', 'even', 'new', 'want', 'because',\n",
        "            'any', 'these', 'give', 'day', 'most', 'us', 'is', 'was', 'are', 'been', 'has', 'had',\n",
        "            'were', 'said', 'each', 'which', 'did', 'very', 'where', 'much', 'too', 'own', 'while'\n",
        "        ])\n",
        "\n",
        "        # Initialize lightweight language model\n",
        "        self.init_language_model()\n",
        "\n",
        "    def init_language_model(self):\n",
        "        \"\"\"Initialize a lightweight language model for coherent text generation\"\"\"\n",
        "        try:\n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            self.text_generator = pipeline(\n",
        "                'text-generation',\n",
        "                model='distilgpt2',\n",
        "                tokenizer=self.tokenizer,\n",
        "                device=0 if self.device == \"cuda\" else -1,\n",
        "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "            )\n",
        "            print(\"Language model initialized successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing language model: {e}\")\n",
        "            self.text_generator = None\n",
        "\n",
        "    def is_valid_english_word(self, word):\n",
        "        \"\"\"Check if a word is likely to be a valid English word\"\"\"\n",
        "        word = word.lower().strip()\n",
        "\n",
        "        # Basic checks\n",
        "        if len(word) < 2:\n",
        "            return False\n",
        "\n",
        "        # Check for common patterns that indicate non-English content\n",
        "        suspicious_patterns = [\n",
        "            r'[0-9]{3,}',  # Long numbers\n",
        "            r'^[a-z]*[0-9]+[a-z]*[0-9]+',  # Mixed letters and numbers\n",
        "            r'^[x-z]{2,}:',  # Protocol-like patterns (xs:, xx:)\n",
        "            r'\\.com|\\.org|\\.net|\\.edu',  # Domain patterns\n",
        "            r'^[a-f0-9]{8,}$',  # Hex strings\n",
        "            r'[^a-z]',  # Non-alphabetic characters (after lowercasing)\n",
        "        ]\n",
        "\n",
        "        for pattern in suspicious_patterns:\n",
        "            if re.search(pattern, word):\n",
        "                return False\n",
        "\n",
        "        # Check if it's a common English word or looks like English\n",
        "        if word in self.common_words:\n",
        "            return True\n",
        "\n",
        "        # Check for basic English word patterns\n",
        "        vowels = set('aeiou')\n",
        "        consonants = set('bcdfghjklmnpqrstvwxyz')\n",
        "\n",
        "        # Must contain at least one vowel (with some exceptions)\n",
        "        if not any(c in vowels for c in word) and word not in ['by', 'my', 'fly', 'try', 'cry', 'dry', 'sky']:\n",
        "            return False\n",
        "\n",
        "        # Should not be all consonants or all vowels (with length > 3)\n",
        "        if len(word) > 3:\n",
        "            if all(c in consonants for c in word) or all(c in vowels for c in word):\n",
        "                return False\n",
        "\n",
        "        # Check for reasonable consonant/vowel distribution\n",
        "        if len(word) > 6:\n",
        "            vowel_count = sum(1 for c in word if c in vowels)\n",
        "            if vowel_count < len(word) * 0.2:  # Less than 20% vowels\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Clean and preprocess text with aggressive filtering\"\"\"\n",
        "        # Remove email addresses, URLs, and suspicious patterns\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "        text = re.sub(r'http\\S+|www\\S+|ftp\\S+', '', text)\n",
        "        text = re.sub(r'[a-z]+://\\S+', '', text)  # Any protocol\n",
        "        text = re.sub(r'\\S*\\.\\S*\\.\\S*', '', text)  # Multiple dots (likely addresses)\n",
        "        text = re.sub(r'[0-9a-f]{8,}', '', text)  # Long hex strings\n",
        "        text = re.sub(r'\\b[a-z]*[0-9]+[a-z0-9]*\\b', '', text)  # Mixed alphanumeric\n",
        "\n",
        "        # Keep basic punctuation for sentence detection\n",
        "        text = re.sub(r'[^a-zA-Z\\s\\.\\!\\?]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
        "\n",
        "        # Filter out non-English words\n",
        "        words = text.split()\n",
        "        filtered_words = [word for word in words if self.is_valid_english_word(word)]\n",
        "\n",
        "        return ' '.join(filtered_words)\n",
        "\n",
        "    def build_enhanced_markov_chains(self, order=2):\n",
        "        \"\"\"Build enhanced Markov chains with aggressive filtering\"\"\"\n",
        "        print(\"Building enhanced Markov chains for each category...\")\n",
        "\n",
        "        for i, category in enumerate(self.categories):\n",
        "            print(f\"Processing category: {category}\")\n",
        "\n",
        "            # Get texts for this category\n",
        "            category_texts = [\n",
        "                self.newsgroups.data[j]\n",
        "                for j, target in enumerate(self.newsgroups.target)\n",
        "                if target == i\n",
        "            ][:100]  # Increase limit for better variety\n",
        "\n",
        "            # Process texts and split into sentences\n",
        "            all_sentences = []\n",
        "            for text in category_texts:\n",
        "                clean_text = self.preprocess_text(text)\n",
        "                if len(clean_text.split()) < 5:  # Skip very short texts\n",
        "                    continue\n",
        "\n",
        "                sentences = [s.strip() for s in re.split(r'[.!?]+', clean_text) if len(s.strip()) > 15]\n",
        "\n",
        "                # Additional filtering for sentences\n",
        "                filtered_sentences = []\n",
        "                for sentence in sentences:\n",
        "                    words = sentence.split()\n",
        "                    # Only keep sentences with reasonable English word ratio\n",
        "                    valid_words = [w for w in words if self.is_valid_english_word(w)]\n",
        "                    if len(valid_words) >= len(words) * 0.8 and len(valid_words) >= 5:\n",
        "                        filtered_sentences.append(' '.join(valid_words))\n",
        "\n",
        "                all_sentences.extend(filtered_sentences)\n",
        "\n",
        "            if len(all_sentences) < 10:  # Need minimum sentences\n",
        "                print(f\"Insufficient clean sentences for {category}, skipping...\")\n",
        "                continue\n",
        "\n",
        "            # Build chains, frequency maps, and sentence markers\n",
        "            chain = defaultdict(lambda: defaultdict(int))\n",
        "            word_freq = defaultdict(int)\n",
        "            starters = []\n",
        "            enders = set()\n",
        "\n",
        "            for sentence in all_sentences[:300]:  # Increase limit\n",
        "                words = sentence.split()\n",
        "                if len(words) < order + 1:\n",
        "                    continue\n",
        "\n",
        "                # Track sentence starters\n",
        "                if len(words) >= order:\n",
        "                    starters.append(tuple(words[:order]))\n",
        "\n",
        "                # Track sentence enders\n",
        "                if len(words) >= 2:\n",
        "                    enders.add(words[-1])\n",
        "\n",
        "                # Build frequency-based chain\n",
        "                for j in range(len(words) - order):\n",
        "                    key = tuple(words[j:j + order])\n",
        "                    next_word = words[j + order]\n",
        "\n",
        "                    if self.is_valid_english_word(next_word):\n",
        "                        chain[key][next_word] += 1\n",
        "                        word_freq[next_word] += 1\n",
        "\n",
        "            # Convert to probability distributions\n",
        "            final_chain = {}\n",
        "            for key, next_words in chain.items():\n",
        "                if len(next_words) > 0:  # Only keep chains with valid next words\n",
        "                    total = sum(next_words.values())\n",
        "                    final_chain[key] = {\n",
        "                        word: count/total\n",
        "                        for word, count in next_words.items()\n",
        "                    }\n",
        "\n",
        "            self.markov_chains[category] = final_chain\n",
        "            self.word_frequencies[category] = dict(word_freq)\n",
        "            self.sentence_starters[category] = starters\n",
        "            self.sentence_enders[category] = list(enders)\n",
        "\n",
        "            print(f\"Built chain for {category}: {len(final_chain)} states, {len(starters)} starters\")\n",
        "\n",
        "    def weighted_choice(self, choices_dict):\n",
        "        \"\"\"Make weighted random choice based on probabilities\"\"\"\n",
        "        if not choices_dict:\n",
        "            return None\n",
        "\n",
        "        words = list(choices_dict.keys())\n",
        "        weights = list(choices_dict.values())\n",
        "\n",
        "        # Add some randomness to prevent always picking the most frequent word\n",
        "        weights = np.array(weights)\n",
        "        weights = np.power(weights, 0.7)  # Reduce dominance of most frequent words\n",
        "        weights = weights / weights.sum()  # Normalize\n",
        "\n",
        "        return np.random.choice(words, p=weights)\n",
        "\n",
        "    def generate_with_improved_markov(self, category, max_length=50, target_sentences=3):\n",
        "        \"\"\"Generate text with improved Markov chain and loop detection\"\"\"\n",
        "        if category not in self.markov_chains:\n",
        "            return \"Breaking news about recent developments in current affairs.\"\n",
        "\n",
        "        chain = self.markov_chains[category]\n",
        "        starters = self.sentence_starters.get(category, [])\n",
        "        enders = self.sentence_enders.get(category, [])\n",
        "\n",
        "        if not chain or not starters:\n",
        "            return \"Breaking news about recent developments in current affairs.\"\n",
        "\n",
        "        result = []\n",
        "        sentences_generated = 0\n",
        "        used_sequences = set()  # Track used 4-grams to prevent loops\n",
        "        attempts = 0\n",
        "        max_attempts = 10\n",
        "\n",
        "        while sentences_generated < target_sentences and len(result) < max_length and attempts < max_attempts:\n",
        "            attempts += 1\n",
        "\n",
        "            # Start new sentence\n",
        "            current_key = random.choice(starters)\n",
        "            sentence_words = list(current_key)\n",
        "\n",
        "            # Generate sentence\n",
        "            for _ in range(min(25, max_length - len(result))):  # Max 25 words per sentence\n",
        "                if current_key not in chain:\n",
        "                    break\n",
        "\n",
        "                # Check for loops\n",
        "                if len(sentence_words) >= 4:\n",
        "                    recent_4gram = tuple(sentence_words[-4:])\n",
        "                    if recent_4gram in used_sequences:\n",
        "                        break\n",
        "                    used_sequences.add(recent_4gram)\n",
        "\n",
        "                # Choose next word using weighted selection\n",
        "                next_word = self.weighted_choice(chain[current_key])\n",
        "                if not next_word or not self.is_valid_english_word(next_word):\n",
        "                    break\n",
        "\n",
        "                sentence_words.append(next_word)\n",
        "\n",
        "                # Check if we should end sentence\n",
        "                if next_word in enders and len(sentence_words) > 6:\n",
        "                    break\n",
        "                elif len(sentence_words) > 20:  # Force sentence end\n",
        "                    break\n",
        "\n",
        "                # Update key for next iteration\n",
        "                current_key = tuple(sentence_words[-2:])\n",
        "\n",
        "            # Add sentence to result if it's substantial\n",
        "            if len(sentence_words) > 5:  # Minimum sentence length\n",
        "                result.extend(sentence_words)\n",
        "                sentences_generated += 1\n",
        "\n",
        "            # Clear old sequences periodically\n",
        "            if len(used_sequences) > 30:\n",
        "                used_sequences.clear()\n",
        "\n",
        "        # Clean up result\n",
        "        text = ' '.join(result[:max_length])\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Ensure it ends properly\n",
        "        if text and not text[-1] in '.!?':\n",
        "            text += '.'\n",
        "\n",
        "        return text\n",
        "\n",
        "    def extract_topic_keywords(self):\n",
        "        \"\"\"Extract key topics and keywords using LDA\"\"\"\n",
        "        print(\"Extracting topic keywords...\")\n",
        "\n",
        "        # Sample texts for faster processing\n",
        "        sample_texts = random.sample(self.newsgroups.data, min(3000, len(self.newsgroups.data)))\n",
        "\n",
        "        # Vectorize texts with better preprocessing\n",
        "        processed_texts = []\n",
        "        for text in sample_texts:\n",
        "            clean_text = self.preprocess_text(text)\n",
        "            if len(clean_text.split()) >= 10:  # Only use substantial texts\n",
        "                processed_texts.append(clean_text)\n",
        "\n",
        "        if len(processed_texts) < 100:\n",
        "            print(\"Warning: Limited clean text available for topic extraction\")\n",
        "            return\n",
        "\n",
        "        tfidf_matrix = self.vectorizer.fit_transform(processed_texts)\n",
        "\n",
        "        # Fit topic model\n",
        "        self.topic_model.fit(tfidf_matrix)\n",
        "\n",
        "        # Extract keywords for each topic\n",
        "        feature_names = self.vectorizer.get_feature_names_out()\n",
        "\n",
        "        for topic_idx, topic in enumerate(self.topic_model.components_):\n",
        "            # Get top keywords and filter for English words\n",
        "            top_indices = topic.argsort()[-15:]  # Get more candidates\n",
        "            top_keywords = [feature_names[i] for i in top_indices if self.is_valid_english_word(feature_names[i])]\n",
        "\n",
        "            self.topic_keywords[self.categories[topic_idx % len(self.categories)]] = top_keywords[-8:]  # Keep top 8\n",
        "\n",
        "        print(\"Topic extraction completed\")\n",
        "\n",
        "    def enhance_with_llm(self, seed_text, category, max_length=250):\n",
        "        \"\"\"Enhance text using language model for better coherence\"\"\"\n",
        "        if not self.text_generator:\n",
        "            return seed_text\n",
        "\n",
        "        try:\n",
        "            # Add category context\n",
        "            prompt = f\"News report about {category.replace('_', ' ')}: {seed_text[:80]}\"\n",
        "\n",
        "            # Generate enhanced text\n",
        "            generated = self.text_generator(\n",
        "                prompt,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                repetition_penalty=1.3  # Stronger repetition penalty\n",
        "            )\n",
        "\n",
        "            enhanced = generated[0]['generated_text'].replace(prompt, '').strip()\n",
        "\n",
        "            # Clean and filter the enhanced text\n",
        "            sentences = enhanced.split('.')\n",
        "            unique_sentences = []\n",
        "            seen = set()\n",
        "\n",
        "            for sentence in sentences:\n",
        "                sentence = sentence.strip()\n",
        "                if sentence and sentence not in seen and len(sentence) > 15:\n",
        "                    # Check if sentence contains mostly English words\n",
        "                    words = sentence.split()\n",
        "                    valid_words = [w for w in words if self.is_valid_english_word(w.lower())]\n",
        "                    if len(valid_words) >= len(words) * 0.8:\n",
        "                        unique_sentences.append(sentence)\n",
        "                        seen.add(sentence)\n",
        "                if len(unique_sentences) >= 4:\n",
        "                    break\n",
        "\n",
        "            result = '. '.join(unique_sentences) + '.'\n",
        "            return result if len(result.split()) >= 30 else seed_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"LLM enhancement failed: {e}\")\n",
        "            return seed_text\n",
        "\n",
        "    def generate_structured_article(self, category, target_length=None):\n",
        "        \"\"\"Generate a structured news article with minimum word count\"\"\"\n",
        "        if target_length is None:\n",
        "            target_length = self.min_word_count\n",
        "\n",
        "        # Generate different parts with controlled lengths\n",
        "        intro = self.generate_with_improved_markov(category, max_length=35, target_sentences=2)\n",
        "        body1 = self.generate_with_improved_markov(category, max_length=40, target_sentences=3)\n",
        "        body2 = self.generate_with_improved_markov(category, max_length=35, target_sentences=2)\n",
        "        body3 = self.generate_with_improved_markov(category, max_length=30, target_sentences=2)\n",
        "\n",
        "        # Combine parts with transitions\n",
        "        transitions = [\"Meanwhile\", \"Furthermore\", \"Additionally\", \"However\", \"In related developments\", \"Moreover\"]\n",
        "        transition1 = random.choice(transitions)\n",
        "        transition2 = random.choice([t for t in transitions if t != transition1])\n",
        "\n",
        "        base_article = f\"{intro} {transition1.lower()}, {body1} {transition2.lower()}, {body2} {body3}\"\n",
        "\n",
        "        # Clean up the article\n",
        "        base_article = re.sub(r'\\s+', ' ', base_article)\n",
        "        base_article = re.sub(r'\\.+', '.', base_article)\n",
        "        base_article = base_article.replace(' .', '.')\n",
        "\n",
        "        # Check word count and extend if necessary\n",
        "        current_word_count = len(base_article.split())\n",
        "\n",
        "        if current_word_count < self.min_word_count:\n",
        "            # Generate additional content\n",
        "            additional_needed = self.min_word_count - current_word_count\n",
        "            extra_content = self.generate_with_improved_markov(\n",
        "                category,\n",
        "                max_length=additional_needed + 20,\n",
        "                target_sentences=max(2, additional_needed // 15)\n",
        "            )\n",
        "            base_article += f\" In conclusion, {extra_content}\"\n",
        "\n",
        "        # Enhance with LLM if available\n",
        "        if self.text_generator and len(base_article.split()) < target_length:\n",
        "            enhanced_article = self.enhance_with_llm(base_article, category, target_length + 50)\n",
        "            final_word_count = len(enhanced_article.split())\n",
        "\n",
        "            if final_word_count >= self.min_word_count:\n",
        "                return enhanced_article\n",
        "\n",
        "        return base_article\n",
        "\n",
        "    def generate_hoax_news(self, num_articles=5, categories=None):\n",
        "        \"\"\"Generate multiple synthetic news articles with minimum word count\"\"\"\n",
        "        if categories is None:\n",
        "            # Filter categories that have sufficient data\n",
        "            valid_categories = [cat for cat in self.categories if cat in self.markov_chains]\n",
        "            if len(valid_categories) < 5:\n",
        "                categories = valid_categories\n",
        "            else:\n",
        "                categories = random.sample(valid_categories, min(8, len(valid_categories)))\n",
        "\n",
        "        print(f\"Generating {num_articles} articles across {len(categories)} categories...\")\n",
        "\n",
        "        articles = []\n",
        "        for i in range(num_articles):\n",
        "            category = random.choice(categories)\n",
        "\n",
        "            print(f\"Generating article {i+1}/{num_articles} for category: {category}\")\n",
        "\n",
        "            # Generate title separately\n",
        "            title_text = self.generate_with_improved_markov(category, max_length=8, target_sentences=1)\n",
        "            title_words = [w for w in title_text.split() if self.is_valid_english_word(w)]\n",
        "            title = ' '.join(title_words[:7]).replace('.', '').title()\n",
        "\n",
        "            # Generate content\n",
        "            content = self.generate_structured_article(category)\n",
        "            word_count = len(content.split())\n",
        "\n",
        "            article = {\n",
        "                'id': i + 1,\n",
        "                'category': category.replace('_', ' ').title(),\n",
        "                'title': title if title else f\"News Report {i+1}\",\n",
        "                'content': content,\n",
        "                'word_count': word_count,\n",
        "                'keywords': self.topic_keywords.get(category, [])[:5]\n",
        "            }\n",
        "\n",
        "            articles.append(article)\n",
        "            print(f\" Generated article with {word_count} words\")\n",
        "\n",
        "        return articles\n",
        "\n",
        "    def train_and_generate(self, num_articles=3):\n",
        "        \"\"\"Main method to train models and generate articles\"\"\"\n",
        "        print(\"Starting enhanced news generation for research purposes...\")\n",
        "        print(f\"Minimum word count per article: {self.min_word_count}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Build improved models\n",
        "        self.build_enhanced_markov_chains()\n",
        "        self.extract_topic_keywords()\n",
        "\n",
        "        # Generate articles\n",
        "        articles = self.generate_hoax_news(num_articles)\n",
        "\n",
        "        return articles\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"Enhanced Synthetic News Generator for Research\")\n",
        "    print(\"=\" * 55)\n",
        "    print(\"WARNING: This tool generates synthetic content for research purposes only!\")\n",
        "    print(\"Features: Advanced filtering, minimum word count, English-only content\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "    # Initialize generator with minimum word count\n",
        "    generator = EnhancedNewsChainGenerator(use_gpu=True, min_word_count=300)\n",
        "\n",
        "    # Generate articles\n",
        "    articles = generator.train_and_generate(num_articles=5)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"GENERATED ARTICLES (FOR RESEARCH ONLY)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    total_words = 0\n",
        "    for article in articles:\n",
        "        print(f\"\\n Article {article['id']}\")\n",
        "        print(f\" Category: {article['category']}\")\n",
        "        print(f\" Title: {article['title']}\")\n",
        "        print(f\" Word Count: {article['word_count']} words\")\n",
        "        print(f\"  Keywords: {', '.join(article['keywords'])}\")\n",
        "        print(\"-\" * 60)\n",
        "        print(f\" Content: {article['content']}\")\n",
        "        print(\"-\" * 70)\n",
        "        total_words += article['word_count']\n",
        "\n",
        "    print(f\"\\n Total words generated: {total_words}\")\n",
        "    print(f\" Average words per article: {total_words/len(articles):.1f}\")\n",
        "\n",
        "    # Save to file\n",
        "    df = pd.DataFrame(articles)\n",
        "    df.to_csv('enhanced_synthetic_news_research.csv', index=False)\n",
        "    print(f\"\\n Results saved to 'enhanced_synthetic_news_research.csv'\")\n",
        "\n",
        "    return articles\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds for reproducibility\n",
        "    # random.seed(42)\n",
        "    # np.random.seed(42)\n",
        "    # torch.manual_seed(42)\n",
        "\n",
        "    articles = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njEe474hWRL6",
        "outputId": "9d2aa209-242b-4123-d87a-130e8fa0ea7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced Synthetic News Generator for Research\n",
            "=======================================================\n",
            "WARNING: This tool generates synthetic content for research purposes only!\n",
            "Features: Advanced filtering, minimum word count, English-only content\n",
            "=======================================================\n",
            "Using CPU\n",
            "Minimum word count per article: 300\n",
            "Loading 20 Newsgroups dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language model initialized successfully\n",
            "Starting enhanced news generation for research purposes...\n",
            "Minimum word count per article: 300\n",
            "============================================================\n",
            "Building enhanced Markov chains for each category...\n",
            "Processing category: alt.atheism\n",
            "Built chain for alt.atheism: 10553 states, 95 starters\n",
            "Processing category: comp.graphics\n",
            "Built chain for comp.graphics: 6854 states, 97 starters\n",
            "Processing category: comp.os.ms-windows.misc\n",
            "Built chain for comp.os.ms-windows.misc: 6310 states, 94 starters\n",
            "Processing category: comp.sys.ibm.pc.hardware\n",
            "Built chain for comp.sys.ibm.pc.hardware: 7733 states, 99 starters\n",
            "Processing category: comp.sys.mac.hardware\n",
            "Built chain for comp.sys.mac.hardware: 5656 states, 98 starters\n",
            "Processing category: comp.windows.x\n",
            "Built chain for comp.windows.x: 9139 states, 97 starters\n",
            "Processing category: misc.forsale\n",
            "Built chain for misc.forsale: 6302 states, 97 starters\n",
            "Processing category: rec.autos\n",
            "Built chain for rec.autos: 8041 states, 96 starters\n",
            "Processing category: rec.motorcycles\n",
            "Built chain for rec.motorcycles: 7424 states, 97 starters\n",
            "Processing category: rec.sport.baseball\n",
            "Built chain for rec.sport.baseball: 8941 states, 94 starters\n",
            "Processing category: rec.sport.hockey\n",
            "Built chain for rec.sport.hockey: 8129 states, 98 starters\n",
            "Processing category: sci.crypt\n",
            "Built chain for sci.crypt: 15543 states, 93 starters\n",
            "Processing category: sci.electronics\n",
            "Built chain for sci.electronics: 8443 states, 97 starters\n",
            "Processing category: sci.med\n",
            "Built chain for sci.med: 9314 states, 95 starters\n",
            "Processing category: sci.space\n",
            "Built chain for sci.space: 13659 states, 99 starters\n",
            "Processing category: soc.religion.christian\n",
            "Built chain for soc.religion.christian: 14848 states, 98 starters\n",
            "Processing category: talk.politics.guns\n",
            "Built chain for talk.politics.guns: 11827 states, 96 starters\n",
            "Processing category: talk.politics.mideast\n",
            "Built chain for talk.politics.mideast: 18523 states, 95 starters\n",
            "Processing category: talk.politics.misc\n",
            "Built chain for talk.politics.misc: 19221 states, 96 starters\n",
            "Processing category: talk.religion.misc\n",
            "Built chain for talk.religion.misc: 12444 states, 95 starters\n",
            "Extracting topic keywords...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=350) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic extraction completed\n",
            "Generating 5 articles across 8 categories...\n",
            "Generating article 1/5 for category: comp.sys.ibm.pc.hardware\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=350) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Generated article with 207 words\n",
            "Generating article 2/5 for category: misc.forsale\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=350) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Generated article with 228 words\n",
            "Generating article 3/5 for category: talk.religion.misc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=350) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Generated article with 161 words\n",
            "Generating article 4/5 for category: sci.electronics\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=350) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Generated article with 169 words\n",
            "Generating article 5/5 for category: sci.crypt\n",
            " Generated article with 153 words\n",
            "\n",
            "======================================================================\n",
            "GENERATED ARTICLES (FOR RESEARCH ONLY)\n",
            "======================================================================\n",
            "\n",
            " Article 1\n",
            " Category: Comp.Sys.Ibm.Pc.Hardware\n",
            " Title: Posted This Once But Didnt Even\n",
            " Word Count: 207 words\n",
            "  Keywords: ibm, hayes, lebanese, cards, fault\n",
            "------------------------------------------------------------\n",
            " Content: the adaptec is best value for dont think the is it possible through either pin configuration or through software programming to. however, have tried numerous combinations to no they work alone or can anyone tell me for the video and thanks to orchid got fix from their tech support hi all netters if upgrade my to. furthermore, that figure for image rotation would seem to hello all you the net are my. hello all you the net are my because of some help with the stealth get. In conclusion, today recieved inwarranty replacement for my diamond speedstar on the its not so easy to exchange these things the one got only does about extra lower thats about properly configured main memory cache will produce better results than caching that figure for isa bus ide was designed to organization compact solutions canberra act australia has anybody every come across problem whereby hard disk scsi on my hello all you the net are my hi often have troubles with my ati ultra plus can handle high res hicolor id love to please someone tell me step by step how to could someone please give me some info on tseng labs et card does anybody have any your help is.\n",
            "----------------------------------------------------------------------\n",
            "\n",
            " Article 2\n",
            " Category: Misc.Forsale\n",
            " Title: Have An Okidata Printer Including Cables\n",
            " Word Count: 228 words\n",
            "  Keywords: faith, medical, day, president, life\n",
            "------------------------------------------------------------\n",
            " Content: well tell us about your stuff being broken or electronics arts ultrabots game for sale inch disks all registration included so you dont get the exactly the same package as. however, have foot switches for they appear to hello my parents are selling foot it olympus stylus pocket sized redeye reduction timer fully time date stamp carrying smallest camera in. meanwhile, radius accelerated graphics supports multiple resolutions and allows onthefly changing of mitsbishi laptop switchable ram installed backlit cga cga mga com ports complete manual set built like tank excellent cosmetic dark gray. am looking out for an inexpensive fax modem card for for sale at each for or look at. In conclusion, im trying to set up personal what im looking for does anyone have pair of sega glasses theyre willing to kirsch pull down window shades white light filtering wide high can be provided upon this is am looking out for an inexpensive fax modem card for tippman special semiauto rifle round feeder inch barrel composite co tank silencer rounds of auto logic panasonic answering machine with dual cassette dolby and input level control for im trying to set up personal what im looking for im looking to find them some nitendo games no it triumph spitfire convertible original miles burgandy color wooden dashboard no rust garage kept heater chrome bumpersnot the black plastic crap brand.\n",
            "----------------------------------------------------------------------\n",
            "\n",
            " Article 3\n",
            " Category: Talk.Religion.Misc\n",
            " Title: So Stop Dodging The What Is\n",
            " Word Count: 161 words\n",
            "  Keywords: thanks, im, like, email, file\n",
            "------------------------------------------------------------\n",
            " Content: kent with regards to the orthodox he is too many errors in your every demand that. in related developments, atoms are not forbidden esfandiar anoushiravani leader of very well and fine but presenting yourself as if you brian am pleased with your and to. however, im not going to let the will of should anyone check lets restrict this to. if one is to post response to very well and fine but presenting yourself as if you. In conclusion, from whose point of most iron alloys and well there is salvation through our lord am christians through ages have had to learn to well where is another false messiah shot down in well am not qualified to assess them and deleted the person who halfheartedly opens the well that would contradict all of this still searching for an im not going to anthony if she doesnt welcome the excruciating pain of well there is huge amount of evidence floating around that.\n",
            "----------------------------------------------------------------------\n",
            "\n",
            " Article 4\n",
            " Category: Sci.Electronics\n",
            " Title: Back In High School Worked As Lab\n",
            " Word Count: 169 words\n",
            "  Keywords: preferably, filter, remove, manager, midi\n",
            "------------------------------------------------------------\n",
            " Content: its not all that but they are safe from nasty dogs because they were carrying piezoelectric just installed motorola fpu in an acid solution decreases the. in related developments, think there is huge difference in the since the condenser since the smaller device will need information on construction and use of. however, the subject though its unfortunately rather short on does lead acid battery discharge and become dead totally unuseable when stored on. live up in british columbia cable company use is called the hello let me introduce problem when measure sinusoidal wave voltage with digital voltmeter makes some kind of. In conclusion, yup its not the designer of the heres something posted about this one but it are circuit boards the material used to two things the level of ions in back in high school worked as lab assistant for bunch of when trying to choose resistor with tolerance better than received reply stating that unless someone else confirms that the well its not the designer of the.\n",
            "----------------------------------------------------------------------\n",
            "\n",
            " Article 5\n",
            " Category: Sci.Crypt\n",
            " Title: The Supreme Power In America Cannot Enforce\n",
            " Word Count: 153 words\n",
            "  Keywords: volt, fan, white, angels, cubs\n",
            "------------------------------------------------------------\n",
            " Content: is it realistic for the serial number is just ask postmaster for the messages in email. moreover, id desparately prefer it if they restrict encryption on the youre blowing qualcomm wants to sell to am writing an article on clipper for. additionally, say they have history of untrustworthy begin signed message meeting of the that. hell slam clinton for anything at all on the directionfinding and directional monitoring can you say little black bakery david. In conclusion, does anyone know what the government will burn few cpuyears trying to ok steve heres sketch of an anonymous id that sternlight your naivete and historical ignorance is drug dealers as others have pointed out since this is how insecure is windows is but thats just the international traffic in the text deleted have my thesis was on sun more to the law enforcement if it is the in what commands you execute and when they.\n",
            "----------------------------------------------------------------------\n",
            "\n",
            " Total words generated: 918\n",
            " Average words per article: 183.6\n",
            "\n",
            " Results saved to 'enhanced_synthetic_news_research.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation per Function"
      ],
      "metadata": {
        "id": "Y474nAM5dOoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialization Phase"
      ],
      "metadata": {
        "id": "781fumk8dQIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedNewsChainGenerator:\n",
        "    def __init__(self, use_gpu=True, min_word_count=150):\n",
        "        \"\"\"\n",
        "        Initialize the enhanced news generator with better filtering\n",
        "        \"\"\"\n",
        "        # Check for GPU availability\n",
        "        if use_gpu and torch.cuda.is_available():\n",
        "            self.device = \"cuda\"\n",
        "            print(f\"Using GPU: {torch.cuda.get_device_name()}\")\n",
        "        elif use_gpu and hasattr(torch, 'xpu') and torch.xpu.is_available():\n",
        "            self.device = \"xpu\"\n",
        "            print(\"Using Intel XPU\")\n",
        "        else:\n",
        "            self.device = \"cpu\"\n",
        "            print(\"Using CPU\")\n",
        "\n",
        "        self.min_word_count = min_word_count\n",
        "        print(f\"Minimum word count per article: {min_word_count}\")\n",
        "\n",
        "        # Load 20 newsgroups data\n",
        "        print(\"Loading 20 Newsgroups dataset...\")\n",
        "        self.newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "        self.categories = self.newsgroups.target_names\n",
        "\n",
        "        # Initialize components\n",
        "        self.vectorizer = TfidfVectorizer(max_features=5000, stop_words='english',\n",
        "                                        min_df=2, max_df=0.8)\n",
        "        self.topic_model = LatentDirichletAllocation(n_components=20, random_state=42)\n",
        "\n",
        "        # Enhanced Markov chains\n",
        "        self.markov_chains = {}\n",
        "        self.word_frequencies = {}\n",
        "        self.sentence_starters = {}\n",
        "        self.sentence_enders = {}\n",
        "        self.topic_keywords = {}\n",
        "\n",
        "        # Common English words for validation\n",
        "        self.common_words = set([\n",
        "            'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'it', 'for', 'not', 'on',\n",
        "            'with', 'he', 'as', 'you', 'do', 'at', 'this', 'but', 'his', 'by', 'from', 'they',\n",
        "            'we', 'say', 'her', 'she', 'or', 'an', 'will', 'my', 'one', 'all', 'would', 'there',\n",
        "            'their', 'what', 'so', 'up', 'out', 'if', 'about', 'who', 'get', 'which', 'go', 'me',\n",
        "            'when', 'make', 'can', 'like', 'time', 'no', 'just', 'him', 'know', 'take', 'people',\n",
        "            'into', 'year', 'your', 'good', 'some', 'could', 'them', 'see', 'other', 'than', 'then',\n",
        "            'now', 'look', 'only', 'come', 'its', 'over', 'think', 'also', 'back', 'after', 'use',\n",
        "            'two', 'how', 'our', 'work', 'first', 'well', 'way', 'even', 'new', 'want', 'because',\n",
        "            'any', 'these', 'give', 'day', 'most', 'us', 'is', 'was', 'are', 'been', 'has', 'had',\n",
        "            'were', 'said', 'each', 'which', 'did', 'very', 'where', 'much', 'too', 'own', 'while'\n",
        "        ])\n",
        "\n",
        "        # Initialize lightweight language model\n",
        "        self.init_language_model()"
      ],
      "metadata": {
        "id": "NDQP6jQveXfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Language Model Preparation"
      ],
      "metadata": {
        "id": "ANQo0kNfePUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_language_model(self):\n",
        "        \"\"\"Initialize a lightweight language model for coherent text generation\"\"\"\n",
        "        try:\n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            self.text_generator = pipeline(\n",
        "                'text-generation',\n",
        "                model='distilgpt2',\n",
        "                tokenizer=self.tokenizer,\n",
        "                device=0 if self.device == \"cuda\" else -1,\n",
        "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "            )\n",
        "            print(\"Language model initialized successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing language model: {e}\")\n",
        "            self.text_generator = None"
      ],
      "metadata": {
        "id": "sX1WUZkweYc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### English Word Validation Engine"
      ],
      "metadata": {
        "id": "NAvEvpYSeb5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_english_word(self, word):\n",
        "        \"\"\"Check if a word is likely to be a valid English word\"\"\"\n",
        "        word = word.lower().strip()\n",
        "\n",
        "        # Basic checks\n",
        "        if len(word) < 2:\n",
        "            return False\n",
        "\n",
        "        # Check for common patterns that indicate non-English content\n",
        "        suspicious_patterns = [\n",
        "            r'[0-9]{3,}',  # Long numbers\n",
        "            r'^[a-z]*[0-9]+[a-z]*[0-9]+',  # Mixed letters and numbers\n",
        "            r'^[x-z]{2,}:',  # Protocol-like patterns (xs:, xx:)\n",
        "            r'\\.com|\\.org|\\.net|\\.edu',  # Domain patterns\n",
        "            r'^[a-f0-9]{8,}$',  # Hex strings\n",
        "            r'[^a-z]',  # Non-alphabetic characters (after lowercasing)\n",
        "        ]\n",
        "\n",
        "        for pattern in suspicious_patterns:\n",
        "            if re.search(pattern, word):\n",
        "                return False\n",
        "\n",
        "        # Check if it's a common English word or looks like English\n",
        "        if word in self.common_words:\n",
        "            return True\n",
        "\n",
        "        # Check for basic English word patterns\n",
        "        vowels = set('aeiou')\n",
        "        consonants = set('bcdfghjklmnpqrstvwxyz')\n",
        "\n",
        "        # Must contain at least one vowel (with some exceptions)\n",
        "        if not any(c in vowels for c in word) and word not in ['by', 'my', 'fly', 'try', 'cry', 'dry', 'sky']:\n",
        "            return False\n",
        "\n",
        "        # Should not be all consonants or all vowels (with length > 3)\n",
        "        if len(word) > 3:\n",
        "            if all(c in consonants for c in word) or all(c in vowels for c in word):\n",
        "                return False\n",
        "\n",
        "        # Check for reasonable consonant/vowel distribution\n",
        "        if len(word) > 6:\n",
        "            vowel_count = sum(1 for c in word if c in vowels)\n",
        "            if vowel_count < len(word) * 0.2:  # Less than 20% vowels\n",
        "                return False\n",
        "\n",
        "        return True"
      ],
      "metadata": {
        "id": "H_f05xkkeiNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Advanced Text Cleaning & Aggressive Filtering"
      ],
      "metadata": {
        "id": "LHFDVeXMed_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(self, text):\n",
        "        \"\"\"Clean and preprocess text with aggressive filtering\"\"\"\n",
        "        # Remove email addresses, URLs, and suspicious patterns\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "        text = re.sub(r'http\\S+|www\\S+|ftp\\S+', '', text)\n",
        "        text = re.sub(r'[a-z]+://\\S+', '', text)  # Any protocol\n",
        "        text = re.sub(r'\\S*\\.\\S*\\.\\S*', '', text)  # Multiple dots (likely addresses)\n",
        "        text = re.sub(r'[0-9a-f]{8,}', '', text)  # Long hex strings\n",
        "        text = re.sub(r'\\b[a-z]*[0-9]+[a-z0-9]*\\b', '', text)  # Mixed alphanumeric\n",
        "\n",
        "        # Keep basic punctuation for sentence detection\n",
        "        text = re.sub(r'[^a-zA-Z\\s\\.\\!\\?]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
        "\n",
        "        # Filter out non-English words\n",
        "        words = text.split()\n",
        "        filtered_words = [word for word in words if self.is_valid_english_word(word)]\n",
        "\n",
        "        return ' '.join(filtered_words)"
      ],
      "metadata": {
        "id": "aSl9WUOweoPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Enhanced Markov Chain Model Training with Probabilistic Weighting"
      ],
      "metadata": {
        "id": "XPkgUqxiepcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_enhanced_markov_chains(self, order=2):\n",
        "        \"\"\"Build enhanced Markov chains with aggressive filtering\"\"\"\n",
        "        print(\"Building enhanced Markov chains for each category...\")\n",
        "\n",
        "        for i, category in enumerate(self.categories):\n",
        "            print(f\"Processing category: {category}\")\n",
        "\n",
        "            # Get texts for this category\n",
        "            category_texts = [\n",
        "                self.newsgroups.data[j]\n",
        "                for j, target in enumerate(self.newsgroups.target)\n",
        "                if target == i\n",
        "            ][:100]  # Increase limit for better variety\n",
        "\n",
        "            # Process texts and split into sentences\n",
        "            all_sentences = []\n",
        "            for text in category_texts:\n",
        "                clean_text = self.preprocess_text(text)\n",
        "                if len(clean_text.split()) < 5:  # Skip very short texts\n",
        "                    continue\n",
        "\n",
        "                sentences = [s.strip() for s in re.split(r'[.!?]+', clean_text) if len(s.strip()) > 15]\n",
        "\n",
        "                # Additional filtering for sentences\n",
        "                filtered_sentences = []\n",
        "                for sentence in sentences:\n",
        "                    words = sentence.split()\n",
        "                    # Only keep sentences with reasonable English word ratio\n",
        "                    valid_words = [w for w in words if self.is_valid_english_word(w)]\n",
        "                    if len(valid_words) >= len(words) * 0.8 and len(valid_words) >= 5:\n",
        "                        filtered_sentences.append(' '.join(valid_words))\n",
        "\n",
        "                all_sentences.extend(filtered_sentences)\n",
        "\n",
        "            if len(all_sentences) < 10:  # Need minimum sentences\n",
        "                print(f\"Insufficient clean sentences for {category}, skipping...\")\n",
        "                continue\n",
        "\n",
        "            # Build chains, frequency maps, and sentence markers\n",
        "            chain = defaultdict(lambda: defaultdict(int))\n",
        "            word_freq = defaultdict(int)\n",
        "            starters = []\n",
        "            enders = set()\n",
        "\n",
        "            for sentence in all_sentences[:300]:  # Increase limit\n",
        "                words = sentence.split()\n",
        "                if len(words) < order + 1:\n",
        "                    continue\n",
        "\n",
        "                # Track sentence starters\n",
        "                if len(words) >= order:\n",
        "                    starters.append(tuple(words[:order]))\n",
        "\n",
        "                # Track sentence enders\n",
        "                if len(words) >= 2:\n",
        "                    enders.add(words[-1])\n",
        "\n",
        "                # Build frequency-based chain\n",
        "                for j in range(len(words) - order):\n",
        "                    key = tuple(words[j:j + order])\n",
        "                    next_word = words[j + order]\n",
        "\n",
        "                    if self.is_valid_english_word(next_word):\n",
        "                        chain[key][next_word] += 1\n",
        "                        word_freq[next_word] += 1\n",
        "\n",
        "            # Convert to probability distributions\n",
        "            final_chain = {}\n",
        "            for key, next_words in chain.items():\n",
        "                if len(next_words) > 0:  # Only keep chains with valid next words\n",
        "                    total = sum(next_words.values())\n",
        "                    final_chain[key] = {\n",
        "                        word: count/total\n",
        "                        for word, count in next_words.items()\n",
        "                    }\n",
        "\n",
        "            self.markov_chains[category] = final_chain\n",
        "            self.word_frequencies[category] = dict(word_freq)\n",
        "            self.sentence_starters[category] = starters\n",
        "            self.sentence_enders[category] = list(enders)\n",
        "\n",
        "            print(f\"Built chain for {category}: {len(final_chain)} states, {len(starters)} starters\")"
      ],
      "metadata": {
        "id": "5TmaIEaSetCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Weighted Random Selection for Natural Text Flow"
      ],
      "metadata": {
        "id": "WR0bn06ZezVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_choice(self, choices_dict):\n",
        "        \"\"\"Make weighted random choice based on probabilities\"\"\"\n",
        "        if not choices_dict:\n",
        "            return None\n",
        "\n",
        "        words = list(choices_dict.keys())\n",
        "        weights = list(choices_dict.values())\n",
        "\n",
        "        # Add some randomness to prevent always picking the most frequent word\n",
        "        weights = np.array(weights)\n",
        "        weights = np.power(weights, 0.7)  # Reduce dominance of most frequent words\n",
        "        weights = weights / weights.sum()  # Normalize\n",
        "\n",
        "        return np.random.choice(words, p=weights)"
      ],
      "metadata": {
        "id": "WO4utOMje1Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Enhanced Topic Modeling & Keyword Extraction"
      ],
      "metadata": {
        "id": "ZwZSCSxce7v3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_topic_keywords(self):\n",
        "        \"\"\"Extract key topics and keywords using LDA\"\"\"\n",
        "        print(\"Extracting topic keywords...\")\n",
        "\n",
        "        # Sample texts for faster processing\n",
        "        sample_texts = random.sample(self.newsgroups.data, min(3000, len(self.newsgroups.data)))\n",
        "\n",
        "        # Vectorize texts with better preprocessing\n",
        "        processed_texts = []\n",
        "        for text in sample_texts:\n",
        "            clean_text = self.preprocess_text(text)\n",
        "            if len(clean_text.split()) >= 10:  # Only use substantial texts\n",
        "                processed_texts.append(clean_text)\n",
        "\n",
        "        if len(processed_texts) < 100:\n",
        "            print(\"Warning: Limited clean text available for topic extraction\")\n",
        "            return\n",
        "\n",
        "        tfidf_matrix = self.vectorizer.fit_transform(processed_texts)\n",
        "\n",
        "        # Fit topic model\n",
        "        self.topic_model.fit(tfidf_matrix)\n",
        "\n",
        "        # Extract keywords for each topic\n",
        "        feature_names = self.vectorizer.get_feature_names_out()\n",
        "\n",
        "        for topic_idx, topic in enumerate(self.topic_model.components_):\n",
        "            # Get top keywords and filter for English words\n",
        "            top_indices = topic.argsort()[-15:]  # Get more candidates\n",
        "            top_keywords = [feature_names[i] for i in top_indices if self.is_valid_english_word(feature_names[i])]\n",
        "\n",
        "            self.topic_keywords[self.categories[topic_idx % len(self.categories)]] = top_keywords[-8:]  # Keep top 8\n",
        "\n",
        "        print(\"Topic extraction completed\")"
      ],
      "metadata": {
        "id": "JbBMQT-8e8Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Advanced Markov Chain Text Generation with Loop Detection"
      ],
      "metadata": {
        "id": "7zOOkX1KfIwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_improved_markov(self, category, max_length=50, target_sentences=3):\n",
        "        \"\"\"Generate text with improved Markov chain and loop detection\"\"\"\n",
        "        if category not in self.markov_chains:\n",
        "            return \"Breaking news about recent developments in current affairs.\"\n",
        "\n",
        "        chain = self.markov_chains[category]\n",
        "        starters = self.sentence_starters.get(category, [])\n",
        "        enders = self.sentence_enders.get(category, [])\n",
        "\n",
        "        if not chain or not starters:\n",
        "            return \"Breaking news about recent developments in current affairs.\"\n",
        "\n",
        "        result = []\n",
        "        sentences_generated = 0\n",
        "        used_sequences = set()  # Track used 4-grams to prevent loops\n",
        "        attempts = 0\n",
        "        max_attempts = 10\n",
        "\n",
        "        while sentences_generated < target_sentences and len(result) < max_length and attempts < max_attempts:\n",
        "            attempts += 1\n",
        "\n",
        "            # Start new sentence\n",
        "            current_key = random.choice(starters)\n",
        "            sentence_words = list(current_key)\n",
        "\n",
        "            # Generate sentence\n",
        "            for _ in range(min(25, max_length - len(result))):  # Max 25 words per sentence\n",
        "                if current_key not in chain:\n",
        "                    break\n",
        "\n",
        "                # Check for loops\n",
        "                if len(sentence_words) >= 4:\n",
        "                    recent_4gram = tuple(sentence_words[-4:])\n",
        "                    if recent_4gram in used_sequences:\n",
        "                        break\n",
        "                    used_sequences.add(recent_4gram)\n",
        "\n",
        "                # Choose next word using weighted selection\n",
        "                next_word = self.weighted_choice(chain[current_key])\n",
        "                if not next_word or not self.is_valid_english_word(next_word):\n",
        "                    break\n",
        "\n",
        "                sentence_words.append(next_word)\n",
        "\n",
        "                # Check if we should end sentence\n",
        "                if next_word in enders and len(sentence_words) > 6:\n",
        "                    break\n",
        "                elif len(sentence_words) > 20:  # Force sentence end\n",
        "                    break\n",
        "\n",
        "                # Update key for next iteration\n",
        "                current_key = tuple(sentence_words[-2:])\n",
        "\n",
        "            # Add sentence to result if it's substantial\n",
        "            if len(sentence_words) > 5:  # Minimum sentence length\n",
        "                result.extend(sentence_words)\n",
        "                sentences_generated += 1\n",
        "\n",
        "            # Clear old sequences periodically\n",
        "            if len(used_sequences) > 30:\n",
        "                used_sequences.clear()\n",
        "\n",
        "        # Clean up result\n",
        "        text = ' '.join(result[:max_length])\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Ensure it ends properly\n",
        "        if text and not text[-1] in '.!?':\n",
        "            text += '.'\n",
        "\n",
        "        return text"
      ],
      "metadata": {
        "id": "DrmlIlYafKbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Enhanced LLM-Based Text Enhancement with Deduplication"
      ],
      "metadata": {
        "id": "gUN9bWRYfPKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def enhance_with_llm(self, seed_text, category, max_length=250):\n",
        "        \"\"\"Enhance text using language model for better coherence\"\"\"\n",
        "        if not self.text_generator:\n",
        "            return seed_text\n",
        "\n",
        "        try:\n",
        "            # Add category context\n",
        "            prompt = f\"News report about {category.replace('_', ' ')}: {seed_text[:80]}\"\n",
        "\n",
        "            # Generate enhanced text\n",
        "            generated = self.text_generator(\n",
        "                prompt,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                repetition_penalty=1.3  # Stronger repetition penalty\n",
        "            )\n",
        "\n",
        "            enhanced = generated[0]['generated_text'].replace(prompt, '').strip()\n",
        "\n",
        "            # Clean and filter the enhanced text\n",
        "            sentences = enhanced.split('.')\n",
        "            unique_sentences = []\n",
        "            seen = set()\n",
        "\n",
        "            for sentence in sentences:\n",
        "                sentence = sentence.strip()\n",
        "                if sentence and sentence not in seen and len(sentence) > 15:\n",
        "                    # Check if sentence contains mostly English words\n",
        "                    words = sentence.split()\n",
        "                    valid_words = [w for w in words if self.is_valid_english_word(w.lower())]\n",
        "                    if len(valid_words) >= len(words) * 0.8:\n",
        "                        unique_sentences.append(sentence)\n",
        "                        seen.add(sentence)\n",
        "                if len(unique_sentences) >= 4:\n",
        "                    break\n",
        "\n",
        "            result = '. '.join(unique_sentences) + '.'\n",
        "            return result if len(result.split()) >= 30 else seed_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"LLM enhancement failed: {e}\")\n",
        "            return seed_text"
      ],
      "metadata": {
        "id": "sCDilMlufXXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Structured Article Assembly with Minimum Word Count Control"
      ],
      "metadata": {
        "id": "WNlacI4ffYEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_structured_article(self, category, target_length=None):\n",
        "        \"\"\"Generate a structured news article with minimum word count\"\"\"\n",
        "        if target_length is None:\n",
        "            target_length = self.min_word_count\n",
        "\n",
        "        # Generate different parts with controlled lengths\n",
        "        intro = self.generate_with_improved_markov(category, max_length=35, target_sentences=2)\n",
        "        body1 = self.generate_with_improved_markov(category, max_length=40, target_sentences=3)\n",
        "        body2 = self.generate_with_improved_markov(category, max_length=35, target_sentences=2)\n",
        "        body3 = self.generate_with_improved_markov(category, max_length=30, target_sentences=2)\n",
        "\n",
        "        # Combine parts with transitions\n",
        "        transitions = [\"Meanwhile\", \"Furthermore\", \"Additionally\", \"However\", \"In related developments\", \"Moreover\"]\n",
        "        transition1 = random.choice(transitions)\n",
        "        transition2 = random.choice([t for t in transitions if t != transition1])\n",
        "\n",
        "        base_article = f\"{intro} {transition1.lower()}, {body1} {transition2.lower()}, {body2} {body3}\"\n",
        "\n",
        "        # Clean up the article\n",
        "        base_article = re.sub(r'\\s+', ' ', base_article)\n",
        "        base_article = re.sub(r'\\.+', '.', base_article)\n",
        "        base_article = base_article.replace(' .', '.')\n",
        "\n",
        "        # Check word count and extend if necessary\n",
        "        current_word_count = len(base_article.split())\n",
        "\n",
        "        if current_word_count < self.min_word_count:\n",
        "            # Generate additional content\n",
        "            additional_needed = self.min_word_count - current_word_count\n",
        "            extra_content = self.generate_with_improved_markov(\n",
        "                category,\n",
        "                max_length=additional_needed + 20,\n",
        "                target_sentences=max(2, additional_needed // 15)\n",
        "            )\n",
        "            base_article += f\" In conclusion, {extra_content}\"\n",
        "\n",
        "        # Enhance with LLM if available\n",
        "        if self.text_generator and len(base_article.split()) < target_length:\n",
        "            enhanced_article = self.enhance_with_llm(base_article, category, target_length + 50)\n",
        "            final_word_count = len(enhanced_article.split())\n",
        "\n",
        "            if final_word_count >= self.min_word_count:\n",
        "                return enhanced_article\n",
        "\n",
        "        return base_article"
      ],
      "metadata": {
        "id": "KDzP0nOCfeMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multi-Article Batch Generation with Quality Control"
      ],
      "metadata": {
        "id": "zIMB_sbvfeo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_hoax_news(self, num_articles=5, categories=None):\n",
        "        \"\"\"Generate multiple synthetic news articles with minimum word count\"\"\"\n",
        "        if categories is None:\n",
        "            # Filter categories that have sufficient data\n",
        "            valid_categories = [cat for cat in self.categories if cat in self.markov_chains]\n",
        "            if len(valid_categories) < 5:\n",
        "                categories = valid_categories\n",
        "            else:\n",
        "                categories = random.sample(valid_categories, min(8, len(valid_categories)))\n",
        "\n",
        "        print(f\"Generating {num_articles} articles across {len(categories)} categories...\")\n",
        "\n",
        "        articles = []\n",
        "        for i in range(num_articles):\n",
        "            category = random.choice(categories)\n",
        "\n",
        "            print(f\"Generating article {i+1}/{num_articles} for category: {category}\")\n",
        "\n",
        "            # Generate title separately\n",
        "            title_text = self.generate_with_improved_markov(category, max_length=8, target_sentences=1)\n",
        "            title_words = [w for w in title_text.split() if self.is_valid_english_word(w)]\n",
        "            title = ' '.join(title_words[:7]).replace('.', '').title()\n",
        "\n",
        "            # Generate content\n",
        "            content = self.generate_structured_article(category)\n",
        "            word_count = len(content.split())\n",
        "\n",
        "            article = {\n",
        "                'id': i + 1,\n",
        "                'category': category.replace('_', ' ').title(),\n",
        "                'title': title if title else f\"News Report {i+1}\",\n",
        "                'content': content,\n",
        "                'word_count': word_count,\n",
        "                'keywords': self.topic_keywords.get(category, [])[:5]\n",
        "            }\n",
        "\n",
        "            articles.append(article)\n",
        "            print(f\" Generated article with {word_count} words\")\n",
        "\n",
        "        return articles"
      ],
      "metadata": {
        "id": "5yH23cgOfiTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Complete Enhanced Pipeline Orchestration"
      ],
      "metadata": {
        "id": "GNt3Tg8CfmJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_generate(self, num_articles=3):\n",
        "        \"\"\"Main method to train models and generate articles\"\"\"\n",
        "        print(\"Starting enhanced news generation for research purposes...\")\n",
        "        print(f\"Minimum word count per article: {self.min_word_count}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Build improved models\n",
        "        self.build_enhanced_markov_chains()\n",
        "        self.extract_topic_keywords()\n",
        "\n",
        "        # Generate articles\n",
        "        articles = self.generate_hoax_news(num_articles)\n",
        "\n",
        "        return articles"
      ],
      "metadata": {
        "id": "BpK7mEKdfn2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main"
      ],
      "metadata": {
        "id": "-jBI7zSrfrTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"Enhanced Synthetic News Generator for Research\")\n",
        "    print(\"=\" * 55)\n",
        "    print(\"WARNING: This tool generates synthetic content for research purposes only!\")\n",
        "    print(\"Features: Advanced filtering, minimum word count, English-only content\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "    # Initialize generator with minimum word count\n",
        "    generator = EnhancedNewsChainGenerator(use_gpu=True, min_word_count=300)\n",
        "\n",
        "    # Generate articles\n",
        "    articles = generator.train_and_generate(num_articles=5)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"GENERATED ARTICLES (FOR RESEARCH ONLY)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    total_words = 0\n",
        "    for article in articles:\n",
        "        print(f\"\\n Article {article['id']}\")\n",
        "        print(f\" Category: {article['category']}\")\n",
        "        print(f\" Title: {article['title']}\")\n",
        "        print(f\" Word Count: {article['word_count']} words\")\n",
        "        print(f\"  Keywords: {', '.join(article['keywords'])}\")\n",
        "        print(\"-\" * 60)\n",
        "        print(f\" Content: {article['content']}\")\n",
        "        print(\"-\" * 70)\n",
        "        total_words += article['word_count']\n",
        "\n",
        "    print(f\"\\n Total words generated: {total_words}\")\n",
        "    print(f\" Average words per article: {total_words/len(articles):.1f}\")\n",
        "\n",
        "    # Save to file\n",
        "    df = pd.DataFrame(articles)\n",
        "    df.to_csv('enhanced_synthetic_news_research.csv', index=False)\n",
        "    print(f\"\\n Results saved to 'enhanced_synthetic_news_research.csv'\")\n",
        "\n",
        "    return articles\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds for reproducibility\n",
        "    # random.seed(42)\n",
        "    # np.random.seed(42)\n",
        "    # torch.manual_seed(42)\n",
        "\n",
        "    articles = main()"
      ],
      "metadata": {
        "id": "NA9XfDKdfqke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Versi 2 (OK)"
      ],
      "metadata": {
        "id": "htizXuxZ3DyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation / Setup (Versi 2)"
      ],
      "metadata": {
        "id": "HHzvp3Ek_dQU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xd8pM3H8zzbM"
      },
      "source": [
        "### Import Library and Setup Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ6ZvQfLzvHx",
        "outputId": "0bde6874-5e98-4242-df24-49113f994c35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "import re\n",
        "import spacy\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load spaCy model for better NLP processing\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "except:\n",
        "    import sys\n",
        "    !{sys.executable} -m spacy download en_core_web_sm\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Load the 20 newsgroups dataset with more relevant categories for news\n",
        "categories = [\n",
        "    'talk.politics.misc',\n",
        "    'talk.politics.guns',\n",
        "    'talk.politics.mideast',\n",
        "    'sci.med',\n",
        "    'sci.space',\n",
        "    'comp.sys.ibm.pc.hardware',\n",
        "    'rec.sport.baseball',\n",
        "    'rec.sport.hockey',\n",
        "    'sci.electronics'\n",
        "]\n",
        "\n",
        "newsgroups = fetch_20newsgroups(\n",
        "    subset='all',\n",
        "    categories=categories,\n",
        "    remove=('headers', 'footers', 'quotes')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfLiGqH7znU8"
      },
      "source": [
        "### Function for Cleaning Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEfcD4Mi3ifA",
        "outputId": "014a46e3-fe06-4059-f0ba-ff1ff69043ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning texts...\n",
            "\n",
            "Total cleaned texts: 7761\n",
            "\n",
            "================================================================================\n",
            "EXAMPLES OF CLEANED TEXTS (First 5):\n",
            "================================================================================\n",
            "\n",
            "--- Example 1 ---\n",
            "misc.legal trimmed Well, that's the obvious conclusion, given your train of logic. The corollary then is that it must be a waste of time for the party to run candidates until the educational program has shown some results. Followups to a.p.l. --\n",
            "Length: 245 characters\n",
            "\n",
            "--- Example 2 ---\n",
            "You can configure devices for the same IRQ as long as you don't use them simultaneously, under Dos at least. Both LPT1: and SB just sit there until you tell them to do something. You can't configure a SoundBlaster for IRQ7 if you got an Ethernet Card which hits that IRQ a thousand times or so per se...\n",
            "Length: 305 characters\n",
            "\n",
            "--- Example 3 ---\n",
            "If Croats are now divided, it is because Croatia seceded from Yugoslavia. Croats in Croatia, B-H, and Serbia were in one country--Yugoslavia-- until they divided themselves. If Muslims are now divided, it is because B-H seceded from Yugoslavia. Muslims in Croatia, B-H, and Serbia were in one country...\n",
            "Length: 573 characters\n",
            "\n",
            "--- Example 4 ---\n",
            "Hmmmm, I'm not sure this is true. According to Mike Lang and good old Stagie, along with the rest of the TV crews in pittsburgh, they winning streak could have stopped because it is a regular season mark. I would think this would also hold with an unbeaten streak for regular season games.\n",
            "Length: 289 characters\n",
            "\n",
            "--- Example 5 ---\n",
            "How about the following scholarly source? Source: Pierre Oberling, \"The Road to Bellapais: The Turkish Cypriot Exodus to Northern Cyprus\", Social Science Monographs, Boulder, 1982, ISBN 88033-000-7. Well, I am forced to disagree with you. The Greeks started massacring the Turkish population on Cypru...\n",
            "Length: 2920 characters\n"
          ]
        }
      ],
      "source": [
        "# Improved cleaning function\n",
        "def clean_text(text):\n",
        "    # Remove email addresses, URLs and normalize whitespace\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove common artifacts from newsgroups data\n",
        "    text = re.sub(r'writes:', '', text)\n",
        "    text = re.sub(r'wrote:', '', text)\n",
        "    text = re.sub(r'saying:', '', text)\n",
        "    text = re.sub(r'>\\s*>', '', text)\n",
        "    text = re.sub(r'>', '', text)\n",
        "\n",
        "    # Remove non-standard characters\n",
        "    text = re.sub(r'[^\\w\\s.,;:!?\\'\"-]', '', text)\n",
        "\n",
        "    # Remove lines that are too short (likely fragments)\n",
        "    lines = text.split('\\n')\n",
        "    lines = [line for line in lines if len(line.strip().split()) > 3]\n",
        "    text = ' '.join(lines)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "print(\"Cleaning texts...\")\n",
        "cleaned_texts = [clean_text(text) for text in newsgroups.data]\n",
        "cleaned_texts = [text for text in cleaned_texts if len(text) > 100]\n",
        "\n",
        "# Print 10 examples of cleaned results\n",
        "print(f\"\\nTotal cleaned texts: {len(cleaned_texts)}\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXAMPLES OF CLEANED TEXTS (First 5):\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, text in enumerate(cleaned_texts[:5], 1):\n",
        "    print(f\"\\n--- Example {i} ---\")\n",
        "    # Print first 300 characters to keep it readable\n",
        "    print(text[:300] + \"...\" if len(text) > 300 else text)\n",
        "    print(f\"Length: {len(text)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Code Generate Hoax (Versi 2 - Basic)"
      ],
      "metadata": {
        "id": "W2ElUo_19SdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Extracting sentences...\")\n",
        "all_sentences = []\n",
        "for text in cleaned_texts:\n",
        "    doc = nlp(text)\n",
        "    for sent in doc.sents:\n",
        "        sent_text = sent.text.strip()\n",
        "        # Filter sentences that are complete and of reasonable length\n",
        "        if len(sent_text.split()) >= 5 and len(sent_text.split()) <= 30:\n",
        "            if sent_text[-1] in '.!?':\n",
        "                if not re.search(r'(^\\W|^\\d+\\.|^>|^I think|^I believe|^However|^But|^And|^Or|^So,|^Also,)', sent_text):\n",
        "                    all_sentences.append(sent_text)\n",
        "\n",
        "print(\"Vectorizing sentences...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_df=0.95, min_df=2,\n",
        "    max_features=1000,\n",
        "    stop_words='english'\n",
        ")\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(all_sentences)\n",
        "\n",
        "print(\"Fitting LDA model...\")\n",
        "n_topics = 15\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=n_topics,\n",
        "    max_iter=10,\n",
        "    learning_method='online',\n",
        "    random_state=42,\n",
        "    batch_size=128,\n",
        "    n_jobs=-1\n",
        ")\n",
        "lda_output = lda.fit_transform(tfidf_matrix)\n",
        "topic_assignments = lda_output.argmax(axis=1)\n",
        "\n",
        "sentences_by_topic = defaultdict(list)\n",
        "for i, topic_idx in enumerate(topic_assignments):\n",
        "    sentences_by_topic[topic_idx].append(all_sentences[i])\n",
        "\n",
        "# Extract keywords for each topic\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "keywords_by_topic = {}\n",
        "\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    top_indices = topic.argsort()[:-11:-1]  # Get top 10 keywords\n",
        "    top_keywords = [feature_names[i] for i in top_indices]\n",
        "    keywords_by_topic[topic_idx] = top_keywords\n",
        "\n",
        "# Define topic names based on keywords\n",
        "topic_names = {}\n",
        "for topic_idx, keywords in keywords_by_topic.items():\n",
        "    topic_names[topic_idx] = ' '.join(keywords[:3])\n",
        "\n",
        "# Create entity database for more realistic entity mentions\n",
        "entity_types = {\n",
        "    'PERSON': set(),\n",
        "    'ORG': set(),\n",
        "    'GPE': set(),  # Countries, cities\n",
        "    'DATE': set()\n",
        "}\n",
        "\n",
        "print(\"Extracting entities...\")\n",
        "# Process a subset to save time\n",
        "for text in random.sample(cleaned_texts, min(1000, len(cleaned_texts))):\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in entity_types and len(ent.text) > 1:\n",
        "            entity_types[ent.label_].add(ent.text)\n",
        "\n",
        "# Template sentences for different parts of an article\n",
        "templates = {\n",
        "    'headline': [\n",
        "        \"BREAKING: {topic} - {keyword} {keyword2} Raises Concerns\",\n",
        "        \"ALERT: New Developments in {topic} as {org} Announces {keyword}\",\n",
        "        \"EXCLUSIVE: {person} Reveals Critical Information About {topic}\",\n",
        "        \"{topic} CRISIS: What {org} Isn't Telling You About {keyword}\",\n",
        "        \"JUST IN: {keyword} {keyword2} Scandal Rocks {org}\"\n",
        "    ],\n",
        "    'intro': [\n",
        "        \"A recent development regarding {topic} has sparked widespread debate.\",\n",
        "        \"New information has emerged about {topic} that demands attention.\",\n",
        "        \"In a stunning revelation, details about {topic} have come to light.\",\n",
        "        \"Reports are circulating about unprecedented events related to {topic}.\"\n",
        "    ],\n",
        "    'quote': [\n",
        "        \"\\\"{sentence}\\\" said {person}, who {verb} the {noun}.\",\n",
        "        \"\\\"{sentence}\\\" according to {person} from {org}.\",\n",
        "        \"{person} stated, \\\"{sentence}\\\" This comes after {org} {verb} the {noun}.\",\n",
        "        \"\\\"{sentence}\\\" {person} explained during the recent {event}.\"\n",
        "    ],\n",
        "    'transition': [\n",
        "        \"Meanwhile, other sources indicate that\",\n",
        "        \"This development comes at a time when\",\n",
        "        \"Experts suggest that this could mean\",\n",
        "        \"Analysis of the situation reveals\",\n",
        "        \"Critics argue that these events demonstrate\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "verbs = [\"addressed\", \"questioned\", \"investigated\", \"analyzed\", \"evaluated\",\n",
        "         \"criticized\", \"supported\", \"endorsed\", \"reviewed\", \"examined\"]\n",
        "nouns = [\"situation\", \"statement\", \"report\", \"findings\", \"allegations\",\n",
        "         \"evidence\", \"controversy\", \"development\", \"policy\", \"decision\"]\n",
        "events = [\"press conference\", \"hearing\", \"investigation\", \"meeting\", \"interview\",\n",
        "          \"briefing\", \"summit\", \"announcement\", \"statement release\"]\n",
        "\n",
        "def generate_improved_headline(topic_idx):\n",
        "    template = random.choice(templates['headline'])\n",
        "    topic = topic_names[topic_idx]\n",
        "    keywords = keywords_by_topic[topic_idx]\n",
        "\n",
        "    return template.format(\n",
        "        topic=topic.title(),\n",
        "        keyword=random.choice(keywords).title(),\n",
        "        keyword2=random.choice([k for k in keywords if k != keywords[0]]).title(),\n",
        "        org=random.choice(list(entity_types['ORG'])) if entity_types['ORG'] else \"Organization\",\n",
        "        person=random.choice(list(entity_types['PERSON'])) if entity_types['PERSON'] else \"Expert\"\n",
        "    )\n",
        "\n",
        "def generate_improved_article():\n",
        "    # Select primary topic for article\n",
        "    primary_topic = random.randint(0, n_topics-1)\n",
        "\n",
        "    # Ensure we have enough sentences for this topic\n",
        "    if len(sentences_by_topic[primary_topic]) < 5:\n",
        "        primary_topic = max(sentences_by_topic.items(), key=lambda x: len(x[1]))[0]\n",
        "\n",
        "    # Generate headline\n",
        "    headline = generate_improved_headline(primary_topic)\n",
        "\n",
        "    # Start building article\n",
        "    article = [headline, \"\"]\n",
        "\n",
        "    # Introduction\n",
        "    intro_template = random.choice(templates['intro'])\n",
        "    article.append(intro_template.format(topic=topic_names[primary_topic]))\n",
        "\n",
        "    # Add 1-2 specific sentences from the topic (RANDOM SELECTION - SIMPLE VERSION)\n",
        "    topic_sentences = sentences_by_topic[primary_topic]\n",
        "    main_sentences = random.sample(topic_sentences, min(2, len(topic_sentences)))\n",
        "    article.extend(main_sentences)\n",
        "    article.append(\"\")\n",
        "\n",
        "    # Add a quote\n",
        "    if topic_sentences:\n",
        "        quote_template = random.choice(templates['quote'])\n",
        "        article.append(quote_template.format(\n",
        "            sentence=random.choice(topic_sentences),\n",
        "            person=random.choice(list(entity_types['PERSON'])) if entity_types['PERSON'] else \"John Smith\",\n",
        "            org=random.choice(list(entity_types['ORG'])) if entity_types['ORG'] else \"Associated Press\",\n",
        "            verb=random.choice(verbs),\n",
        "            noun=random.choice(nouns),\n",
        "            event=random.choice(events)\n",
        "        ))\n",
        "\n",
        "    # Add a secondary perspective (from related topic)\n",
        "    related_topics = list(range(n_topics))\n",
        "    related_topics.remove(primary_topic)\n",
        "    if related_topics:\n",
        "        secondary_topic = random.choice(related_topics)\n",
        "        if sentences_by_topic[secondary_topic]:\n",
        "            article.append(\"\")\n",
        "            article.append(random.choice(templates['transition']))\n",
        "            secondary_sentences = random.sample(sentences_by_topic[secondary_topic],\n",
        "                                               min(2, len(sentences_by_topic[secondary_topic])))\n",
        "            article.extend(secondary_sentences)\n",
        "\n",
        "    # Conclusion\n",
        "    if topic_sentences:\n",
        "        article.append(\"\")\n",
        "        article.append(random.choice(topic_sentences))\n",
        "\n",
        "    return \"\\n\".join(article)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING IMPROVED SYNTHETIC NEWS ARTICLES (FOR RESEARCH PURPOSES ONLY)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"\\nARTICLE #{i+1}:\")\n",
        "    print(\"=\"*50)\n",
        "    article = generate_improved_article()\n",
        "    print(article)\n",
        "    print(\"=\"*50)\n",
        "\n",
        "print(\"\\nDISCLAIMER: These articles are entirely synthetic and generated for research purposes.\")\n",
        "print(\"They may contain factual inaccuracies and should not be shared as real news.\")"
      ],
      "metadata": {
        "id": "FmsN6kYr9Xtl",
        "outputId": "2d86e6b5-947b-4bb5-c1c9-9e37b7a6ad41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting sentences...\n",
            "Vectorizing sentences...\n",
            "Fitting LDA model...\n",
            "Extracting entities...\n",
            "\n",
            "================================================================================\n",
            "GENERATING IMPROVED SYNTHETIC NEWS ARTICLES (FOR RESEARCH PURPOSES ONLY)\n",
            "================================================================================\n",
            "\n",
            "ARTICLE #1:\n",
            "==================================================\n",
            "JUST IN: Doing Using Scandal Rocks the Armenian Government\n",
            "\n",
            "A recent development regarding does help little has sparked widespread debate.\n",
            "The Greeks I mentioned who wouldn't talk to me are educated people.\n",
            "They can quote you a price and tell you where to send your money.\n",
            "\n",
            "\"I wonder how many American-owned companies employ those in Central Ohio?\" said ya, who addressed the statement.\n",
            "\n",
            "Meanwhile, other sources indicate that\n",
            "Turks and Kurds demand the right to return to their lands, to determine their own future as a nation in their own homeland.\n",
            "So It will be nice if Yigal will tell us: 1.\n",
            "\n",
            "So another strong game from Tommy Soderstrom who hadn't been tested much in his last couple of starts.\n",
            "==================================================\n",
            "\n",
            "ARTICLE #2:\n",
            "==================================================\n",
            "BREAKING: Use Year Let - Day Thing Raises Concerns\n",
            "\n",
            "New information has emerged about use year let that demands attention.\n",
            "I flicked the Penguins game on briefly and saw : Ulf cross-check Valeri in the face.\n",
            "This breaks the isolation between the two clocks that used to ensure that errors in the DOS clock did not bleed over into the BIOS clock.\n",
            "\n",
            "Cincy stated, \"The point is that the land was sold legally, often at prices above its actual value.\" This comes after the Texas Rangers examined the findings.\n",
            "\n",
            "Analysis of the situation reveals\n",
            "The government however, didn't give them 1billion for the developement of a full scale rocket.\n",
            "The user will receive an image once every 1 to 4 seconds depending on the speed of their data link to LTM1.\n",
            "\n",
            "Who will be the stars this year?\n",
            "==================================================\n",
            "\n",
            "ARTICLE #3:\n",
            "==================================================\n",
            "BREAKING: Like Actually Seen - Like Source Raises Concerns\n",
            "\n",
            "New information has emerged about like actually seen that demands attention.\n",
            "If you do NOT like seeing letters such as this one on your newsgroup direct all complaints to my instructor at -- \"Kilimanjaro is a pretty tricky climb.\n",
            "Leeman wouldn't take it and when they tried to give it back to Marsh, he wouldn't take it neither.\n",
            "\n",
            "\"What kind of outlets do I need in a kitchen?\" according to Sabbath from Baseball America.\n",
            "\n",
            "Experts suggest that this could mean\n",
            "Am I justified in being pissed off at this doctor?\n",
            "I must be missing something.\n",
            "\n",
            "With these, \"high-thrust\" interplanetary flight is not possible, because system acceleration at capabilities are always less than the local gravitational acceleration.\n",
            "==================================================\n",
            "\n",
            "DISCLAIMER: These articles are entirely synthetic and generated for research purposes.\n",
            "They may contain factual inaccuracies and should not be shared as real news.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation per Function :"
      ],
      "metadata": {
        "id": "VMFuipci9uO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extraction to Generate"
      ],
      "metadata": {
        "id": "9oyGnmiEyTnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all high-quality sentences\n",
        "print(\"Extracting sentences...\")\n",
        "all_sentences = []\n",
        "for text in cleaned_texts:\n",
        "    doc = nlp(text)\n",
        "    for sent in doc.sents:\n",
        "        sent_text = sent.text.strip()\n",
        "        # Filter sentences that are complete and of reasonable length\n",
        "        if len(sent_text.split()) >= 5 and len(sent_text.split()) <= 30:\n",
        "            if sent_text[-1] in '.!?':\n",
        "                if not re.search(r'(^\\W|^\\d+\\.|^>|^I think|^I believe|^However|^But|^And|^Or|^So,|^Also,)', sent_text):\n",
        "                    all_sentences.append(sent_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h5LTzl-yY33",
        "outputId": "9bad1d7a-14f2-419a-954c-ea0a94ce50cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting sentences...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vectorization & Topic Modeling"
      ],
      "metadata": {
        "id": "fFvLAzyeK624"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vectorizing sentences...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_df=0.95, min_df=2,\n",
        "    max_features=1000,\n",
        "    stop_words='english'\n",
        ")\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(all_sentences)\n",
        "\n",
        "print(\"Fitting LDA model...\")\n",
        "n_topics = 15\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=n_topics,\n",
        "    max_iter=10,\n",
        "    learning_method='online',\n",
        "    random_state=42,\n",
        "    batch_size=128,\n",
        "    n_jobs=-1\n",
        ")\n",
        "lda_output = lda.fit_transform(tfidf_matrix)\n",
        "topic_assignments = lda_output.argmax(axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY9gGwGfK7Zz",
        "outputId": "b59319c7-7fe5-4487-dd8e-5d4081289959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorizing sentences...\n",
            "Fitting LDA model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Grouping Sentences by Topic"
      ],
      "metadata": {
        "id": "4mJZU2ZS6ncw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group sentences by topic\n",
        "sentences_by_topic = defaultdict(list)\n",
        "for i, topic_idx in enumerate(topic_assignments):\n",
        "    sentences_by_topic[topic_idx].append(all_sentences[i])"
      ],
      "metadata": {
        "id": "FPIWr4Op6xmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extracting the main keywords from each topic"
      ],
      "metadata": {
        "id": "iQi7luFG-wBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract keywords for each topic\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "keywords_by_topic = {}\n",
        "\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    top_indices = topic.argsort()[:-11:-1]  # Get top 10 keywords\n",
        "    top_keywords = [feature_names[i] for i in top_indices]\n",
        "    keywords_by_topic[topic_idx] = top_keywords\n",
        "\n",
        "# Define topic names based on keywords\n",
        "topic_names = {}\n",
        "for topic_idx, keywords in keywords_by_topic.items():\n",
        "    topic_names[topic_idx] = ' '.join(keywords[:3])"
      ],
      "metadata": {
        "id": "Sh_Nd61_-wBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building Entity Database"
      ],
      "metadata": {
        "id": "WkUfRXpy8Jio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create entity database for more realistic entity mentions\n",
        "entity_types = {\n",
        "    'PERSON': set(),\n",
        "    'ORG': set(),\n",
        "    'GPE': set(),  # Countries, cities\n",
        "    'DATE': set()\n",
        "}\n",
        "\n",
        "print(\"Extracting entities...\")\n",
        "# Process a subset to save time\n",
        "for text in random.sample(cleaned_texts, min(1000, len(cleaned_texts))):\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in entity_types and len(ent.text) > 1:\n",
        "            entity_types[ent.label_].add(ent.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxPyUJDg8Nly",
        "outputId": "52d56cbd-e4cb-4f82-c37e-ab05caf26f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting entities...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Template and Helpers"
      ],
      "metadata": {
        "id": "YNTvrO-7Lmbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Template sentences for different parts of an article\n",
        "templates = {\n",
        "    'headline': [\n",
        "        \"BREAKING: {topic} - {keyword} {keyword2} Raises Concerns\",\n",
        "        \"ALERT: New Developments in {topic} as {org} Announces {keyword}\",\n",
        "        \"EXCLUSIVE: {person} Reveals Critical Information About {topic}\",\n",
        "        \"{topic} CRISIS: What {org} Isn't Telling You About {keyword}\",\n",
        "        \"JUST IN: {keyword} {keyword2} Scandal Rocks {org}\"\n",
        "    ],\n",
        "    'intro': [\n",
        "        \"A recent development regarding {topic} has sparked widespread debate.\",\n",
        "        \"New information has emerged about {topic} that demands attention.\",\n",
        "        \"In a stunning revelation, details about {topic} have come to light.\",\n",
        "        \"Reports are circulating about unprecedented events related to {topic}.\"\n",
        "    ],\n",
        "    'quote': [\n",
        "        \"\\\"{sentence}\\\" said {person}, who {verb} the {noun}.\",\n",
        "        \"\\\"{sentence}\\\" according to {person} from {org}.\",\n",
        "        \"{person} stated, \\\"{sentence}\\\" This comes after {org} {verb} the {noun}.\",\n",
        "        \"\\\"{sentence}\\\" {person} explained during the recent {event}.\"\n",
        "    ],\n",
        "    'transition': [\n",
        "        \"Meanwhile, other sources indicate that\",\n",
        "        \"This development comes at a time when\",\n",
        "        \"Experts suggest that this could mean\",\n",
        "        \"Analysis of the situation reveals\",\n",
        "        \"Critics argue that these events demonstrate\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "verbs = [\"addressed\", \"questioned\", \"investigated\", \"analyzed\", \"evaluated\",\n",
        "         \"criticized\", \"supported\", \"endorsed\", \"reviewed\", \"examined\"]\n",
        "nouns = [\"situation\", \"statement\", \"report\", \"findings\", \"allegations\",\n",
        "         \"evidence\", \"controversy\", \"development\", \"policy\", \"decision\"]\n",
        "events = [\"press conference\", \"hearing\", \"investigation\", \"meeting\", \"interview\",\n",
        "          \"briefing\", \"summit\", \"announcement\", \"statement release\"]\n",
        "\n",
        "# =====================================================================================\n",
        "# 7. GENERATION FUNCTIONS (SIMPLE VERSION - RANDOM SENTENCES)\n",
        "# =====================================================================================\n",
        "def generate_improved_headline(topic_idx):\n",
        "    template = random.choice(templates['headline'])\n",
        "    topic = topic_names[topic_idx]\n",
        "    keywords = keywords_by_topic[topic_idx]\n",
        "\n",
        "    return template.format(\n",
        "        topic=topic.title(),\n",
        "        keyword=random.choice(keywords).title(),\n",
        "        keyword2=random.choice([k for k in keywords if k != keywords[0]]).title(),\n",
        "        org=random.choice(list(entity_types['ORG'])) if entity_types['ORG'] else \"Organization\",\n",
        "        person=random.choice(list(entity_types['PERSON'])) if entity_types['PERSON'] else \"Expert\"\n",
        "    )\n",
        "\n",
        "def generate_improved_article():\n",
        "    # Select primary topic for article\n",
        "    primary_topic = random.randint(0, n_topics-1)\n",
        "\n",
        "    # Ensure we have enough sentences for this topic\n",
        "    if len(sentences_by_topic[primary_topic]) < 5:\n",
        "        primary_topic = max(sentences_by_topic.items(), key=lambda x: len(x[1]))[0]\n",
        "\n",
        "    # Generate headline\n",
        "    headline = generate_improved_headline(primary_topic)\n",
        "\n",
        "    # Start building article\n",
        "    article = [headline, \"\"]\n",
        "\n",
        "    # Introduction\n",
        "    intro_template = random.choice(templates['intro'])\n",
        "    article.append(intro_template.format(topic=topic_names[primary_topic]))\n",
        "\n",
        "    # Add 1-2 specific sentences from the topic (RANDOM SELECTION - SIMPLE VERSION)\n",
        "    topic_sentences = sentences_by_topic[primary_topic]\n",
        "    main_sentences = random.sample(topic_sentences, min(2, len(topic_sentences)))\n",
        "    article.extend(main_sentences)\n",
        "    article.append(\"\")\n",
        "\n",
        "    # Add a quote\n",
        "    if topic_sentences:\n",
        "        quote_template = random.choice(templates['quote'])\n",
        "        article.append(quote_template.format(\n",
        "            sentence=random.choice(topic_sentences),\n",
        "            person=random.choice(list(entity_types['PERSON'])) if entity_types['PERSON'] else \"John Smith\",\n",
        "            org=random.choice(list(entity_types['ORG'])) if entity_types['ORG'] else \"Associated Press\",\n",
        "            verb=random.choice(verbs),\n",
        "            noun=random.choice(nouns),\n",
        "            event=random.choice(events)\n",
        "        ))\n",
        "\n",
        "    # Add a secondary perspective (from related topic)\n",
        "    related_topics = list(range(n_topics))\n",
        "    related_topics.remove(primary_topic)\n",
        "    if related_topics:\n",
        "        secondary_topic = random.choice(related_topics)\n",
        "        if sentences_by_topic[secondary_topic]:\n",
        "            article.append(\"\")\n",
        "            article.append(random.choice(templates['transition']))\n",
        "            secondary_sentences = random.sample(sentences_by_topic[secondary_topic],\n",
        "                                               min(2, len(sentences_by_topic[secondary_topic])))\n",
        "            article.extend(secondary_sentences)\n",
        "\n",
        "    # Conclusion\n",
        "    if topic_sentences:\n",
        "        article.append(\"\")\n",
        "        article.append(random.choice(topic_sentences))\n",
        "\n",
        "    return \"\\n\".join(article)"
      ],
      "metadata": {
        "id": "iSFoEKdhLouL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa1dShy2IkEH"
      },
      "source": [
        "#### Generate Article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhZDmjtFvuNe",
        "outputId": "fbc3eae7-a69c-4e9c-f235-7cecb440e157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "GENERATING IMPROVED SYNTHETIC NEWS ARTICLES (FOR RESEARCH PURPOSES ONLY)\n",
            "================================================================================\n",
            "\n",
            "ARTICLE #1:\n",
            "==================================================\n",
            "ALERT: New Developments in Know Said Right as International Technical Allied Signal, Inc. Announces Better\n",
            "\n",
            "In a stunning revelation, details about know said right have come to light.\n",
            "Also Bonds is less in need of protection behind him because he is such a good base stealer a walk is a potential double.\n",
            "Speaking of Roger and ilk, whatever happened to good ol' gln?\n",
            "\n",
            "\"They sure get shrill whenever their belief structure is being shaken. :  :  Kinda reminds you of the BDs, doesn't it? :  :  Jim : Go to hell.\" said Steve Foster 1-2, who questioned the report.\n",
            "\n",
            "Meanwhile, other sources indicate that\n",
            "From memory it has an 80 MB hard drive.\n",
            "I certainly didn't, but I'd have a hard time arguing against him at this point.\n",
            "\n",
            "You should have a sketch or detailed drawing of what you plan on doing.\n",
            "==================================================\n",
            "\n",
            "ARTICLE #2:\n",
            "==================================================\n",
            "JUST IN: Space Isn Scandal Rocks Energy\n",
            "\n",
            "In a stunning revelation, details about did used space have come to light.\n",
            "Now, Space Marketing is working with University of Colorado and Livermore engineers on a plan to place a mile-long inflatable billboard in low-earth orbit.\n",
            "It would also enable us to absorb more young people coming into the work force in jobs that otherwise will not be created.\n",
            "\n",
            "\"At this kind of speed slow, by digital standards such an adder is much less expensive than analog components of comparable precision.\" Anna Vasilyevna explained during the recent press conference.\n",
            "\n",
            "Experts suggest that this could mean\n",
            "Should Prozac be taken off the market because long-term effects, if any, are not known?\n",
            "Unfortunately for him, because his book parades itself as \"scholarly,\" he is forced to put footnotes.\n",
            "\n",
            "The Sabres seem ready to put in the extra work.\n",
            "==================================================\n",
            "\n",
            "ARTICLE #3:\n",
            "==================================================\n",
            "ALERT: New Developments in Game Number Probably as Eklund Announces Far\n",
            "\n",
            "New information has emerged about game number probably that demands attention.\n",
            "Many of them shot dead from their heads as close as less than 1 meter.\n",
            "The wires are run along side structural members eg: joists or studs using ceramic stand-offs knobs.\n",
            "\n",
            "\"Okay here is what I have so far: Have a group any size, preferibly small, but?\" Craig explained during the recent investigation.\n",
            "\n",
            "Meanwhile, other sources indicate that\n",
            "Have anyone heard about it or using it?\n",
            "Collective and inflicted on unarmed innocents just not as through.\n",
            "\n",
            "We were also wounded, there was blood, and we were scratched all over--we got our share.\n",
            "==================================================\n",
            "\n",
            "DISCLAIMER: These articles are entirely synthetic and generated for research purposes.\n",
            "They may contain factual inaccuracies and should not be shared as real news.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING IMPROVED SYNTHETIC NEWS ARTICLES (FOR RESEARCH PURPOSES ONLY)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"\\nARTICLE #{i+1}:\")\n",
        "    print(\"=\"*50)\n",
        "    article = generate_improved_article()\n",
        "    print(article)\n",
        "    print(\"=\"*50)\n",
        "\n",
        "print(\"\\nDISCLAIMER: These articles are entirely synthetic and generated for research purposes.\")\n",
        "print(\"They may contain factual inaccuracies and should not be shared as real news.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKEz5E_LQVYc"
      },
      "source": [
        "## Full Code Generate Hoax (Versi 2 - Advanced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAHtMjk1Qavx",
        "outputId": "2435f68f-320a-4658-94db-e1200b199f78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting sentences by document...\n",
            "Vectorizing sentences...\n",
            "Fitting LDA model...\n",
            "Extracting entities...\n",
            "\n",
            "================================================================================\n",
            "GENERATING ADVANCED SYNTHETIC NEWS ARTICLES (FOR RESEARCH PURPOSES ONLY)\n",
            "================================================================================\n",
            "\n",
            "ARTICLE #1:\n",
            "==================================================\n",
            "JUST IN: Didn Said Scandal Rocks Space Technology a.k.a\n",
            "\n",
            "A recent development regarding know said right has sparked widespread debate.\n",
            "How do I know who's who and what's what?\n",
            "Lyuda said that she was Russian, they said, we'll let you go, we aren't touching the Russians, go.\n",
            "I can't remember my supervisor's telephone number, but something had to be done.\n",
            "He didn't have any idea what was going on.\n",
            "He's the head of our administration.\n",
            "\n",
            "\"Lyuda said that she was Russian, they said, we'll let you go, we aren't touching the Russians, go.\" said Kolly Gyshlag, who addressed the policy.\n",
            "\n",
            "Critics argue that these events demonstrate\n",
            "They only managed to run up there and grab something one time.\n",
            "I tried to call my boss.\n",
            "\n",
            "Further updates will be provided as the situation develops.\n",
            "==================================================\n",
            "\n",
            "ARTICLE #2:\n",
            "==================================================\n",
            "ALERT: New Developments in Did Used Space as COURTS Announces Mean\n",
            "\n",
            "Reports are circulating about unprecedented events related to did used space.\n",
            "This is one of the items that changes most often.\n",
            "There are very different requirements for mobile homes.\n",
            "The CEC is slightly different.\n",
            "There are now fixtures that contain integral thermal cutouts and fairly large cases that can be buried directly in insulation.\n",
            "They are usually limited to 75 watt bulbs, and are unfortunately, somewhat more expensive than the older types.\n",
            "\n",
            "Dee Dee stated, \"This is one of the items that changes most often.\" This comes after COURTS evaluated the controversy.\n",
            "\n",
            "Experts suggest that this could mean\n",
            "I don't think that's true.\n",
            "He went looking for us.\n",
            "\n",
            "Further updates will be provided as the situation develops.\n",
            "==================================================\n",
            "\n",
            "ARTICLE #3:\n",
            "==================================================\n",
            "Did Used Space CRISIS: What MECL System Design Handbook Isn't Telling You About Long\n",
            "\n",
            "Reports are circulating about unprecedented events related to did used space.\n",
            "There are now fixtures that contain integral thermal cutouts and fairly large cases that can be buried directly in insulation.\n",
            "They are usually limited to 75 watt bulbs, and are unfortunately, somewhat more expensive than the older types.\n",
            "What does it mean when the lights brighten when a motor starts?\n",
            "There are 3 phase circuits with different voltages.\n",
            "Bringing in a 3 phase feed to your house is usually ridiculously expensive, or impossible.\n",
            "\n",
            "\"There are 3 phase circuits with different voltages.\" Randy WeaverKevin Harris explained during the recent hearing.\n",
            "\n",
            "Analysis of the situation reveals\n",
            "Ottawa Senators The Ottawa Senators received the go-ahead to build the 18,500-seat Palladium on the proposed location in nearby Kanata, Ont.\n",
            "Traded veteran right wing Greg Paslawski to the Calgary Flames for future considerations.\n",
            "\n",
            "Further updates will be provided as the situation develops.\n",
            "==================================================\n",
            "\n",
            "DISCLAIMER: These articles are entirely synthetic and generated for research purposes.\n",
            "They may contain factual inaccuracies and should not be shared as real news.\n"
          ]
        }
      ],
      "source": [
        "print(\"Extracting sentences by document...\")\n",
        "all_sentences = []\n",
        "doc_sentence_mapping = []  # Track which sentence belongs to which document\n",
        "\n",
        "for doc_idx, text in enumerate(cleaned_texts):\n",
        "    doc = nlp(text)\n",
        "    doc_sentences = []\n",
        "    for sent in doc.sents:\n",
        "        sent_text = sent.text.strip()\n",
        "        # Filter sentences that are complete and of reasonable length\n",
        "        if len(sent_text.split()) >= 5 and len(sent_text.split()) <= 30:\n",
        "            if sent_text[-1] in '.!?':\n",
        "                if not re.search(r'(^\\W|^\\d+\\.|^>|^I think|^I believe|^However|^But|^And|^Or|^So,|^Also,)', sent_text):\n",
        "                    all_sentences.append(sent_text)\n",
        "                    doc_sentences.append(sent_text)\n",
        "                    doc_sentence_mapping.append(doc_idx)\n",
        "\n",
        "print(\"Vectorizing sentences...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_df=0.95, min_df=2,\n",
        "    max_features=1000,\n",
        "    stop_words='english'\n",
        ")\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(all_sentences)\n",
        "\n",
        "print(\"Fitting LDA model...\")\n",
        "n_topics = 15\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=n_topics,\n",
        "    max_iter=10,\n",
        "    learning_method='online',\n",
        "    random_state=42,\n",
        "    batch_size=128,\n",
        "    n_jobs=-1\n",
        ")\n",
        "lda_output = lda.fit_transform(tfidf_matrix)\n",
        "topic_assignments = lda_output.argmax(axis=1)\n",
        "\n",
        "sentences_by_topic = defaultdict(list)\n",
        "sentences_by_topic_doc = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "for i, topic_idx in enumerate(topic_assignments):\n",
        "    sentence = all_sentences[i]\n",
        "    doc_idx = doc_sentence_mapping[i]\n",
        "\n",
        "    sentences_by_topic[topic_idx].append(sentence)\n",
        "    sentences_by_topic_doc[topic_idx][doc_idx].append(sentence)\n",
        "\n",
        "# Extract keywords for each topic\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "keywords_by_topic = {}\n",
        "\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    top_indices = topic.argsort()[:-11:-1]  # Get top 10 keywords\n",
        "    top_keywords = [feature_names[i] for i in top_indices]\n",
        "    keywords_by_topic[topic_idx] = top_keywords\n",
        "\n",
        "# Define topic names based on keywords\n",
        "topic_names = {}\n",
        "for topic_idx, keywords in keywords_by_topic.items():\n",
        "    topic_names[topic_idx] = ' '.join(keywords[:3])\n",
        "\n",
        "# Create entity database for more realistic entity mentions\n",
        "entity_types = {\n",
        "    'PERSON': set(),\n",
        "    'ORG': set(),\n",
        "    'GPE': set(),  # Countries, cities\n",
        "    'DATE': set()\n",
        "}\n",
        "\n",
        "print(\"Extracting entities...\")\n",
        "# Process a subset to save time\n",
        "for text in random.sample(cleaned_texts, min(1000, len(cleaned_texts))):\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in entity_types and len(ent.text) > 1:\n",
        "            entity_types[ent.label_].add(ent.text)\n",
        "\n",
        "# Template sentences for different parts of an article\n",
        "templates = {\n",
        "    'headline': [\n",
        "        \"BREAKING: {topic} - {keyword} {keyword2} Raises Concerns\",\n",
        "        \"ALERT: New Developments in {topic} as {org} Announces {keyword}\",\n",
        "        \"EXCLUSIVE: {person} Reveals Critical Information About {topic}\",\n",
        "        \"{topic} CRISIS: What {org} Isn't Telling You About {keyword}\",\n",
        "        \"JUST IN: {keyword} {keyword2} Scandal Rocks {org}\"\n",
        "    ],\n",
        "    'intro': [\n",
        "        \"A recent development regarding {topic} has sparked widespread debate.\",\n",
        "        \"New information has emerged about {topic} that demands attention.\",\n",
        "        \"In a stunning revelation, details about {topic} have come to light.\",\n",
        "        \"Reports are circulating about unprecedented events related to {topic}.\"\n",
        "    ],\n",
        "    'quote': [\n",
        "        \"\\\"{sentence}\\\" said {person}, who {verb} the {noun}.\",\n",
        "        \"\\\"{sentence}\\\" according to {person} from {org}.\",\n",
        "        \"{person} stated, \\\"{sentence}\\\" This comes after {org} {verb} the {noun}.\",\n",
        "        \"\\\"{sentence}\\\" {person} explained during the recent {event}.\"\n",
        "    ],\n",
        "    'transition': [\n",
        "        \"Meanwhile, other sources indicate that\",\n",
        "        \"This development comes at a time when\",\n",
        "        \"Experts suggest that this could mean\",\n",
        "        \"Analysis of the situation reveals\",\n",
        "        \"Critics argue that these events demonstrate\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "verbs = [\"addressed\", \"questioned\", \"investigated\", \"analyzed\", \"evaluated\",\n",
        "         \"criticized\", \"supported\", \"endorsed\", \"reviewed\", \"examined\"]\n",
        "nouns = [\"situation\", \"statement\", \"report\", \"findings\", \"allegations\",\n",
        "         \"evidence\", \"controversy\", \"development\", \"policy\", \"decision\"]\n",
        "events = [\"press conference\", \"hearing\", \"investigation\", \"meeting\", \"interview\",\n",
        "          \"briefing\", \"summit\", \"announcement\", \"statement release\"]\n",
        "\n",
        "def generate_improved_headline(topic_idx, person=None, org=None):\n",
        "    template = random.choice(templates['headline'])\n",
        "    topic = topic_names[topic_idx]\n",
        "    keywords = keywords_by_topic[topic_idx]\n",
        "    keyword_choice = random.sample(keywords, 2) if len(keywords) >= 2 else [keywords[0], keywords[0]]\n",
        "    org = org or (random.choice(list(entity_types['ORG'])) if entity_types['ORG'] else \"Organization\")\n",
        "    person = person or (random.choice(list(entity_types['PERSON'])) if entity_types['PERSON'] else \"Expert\")\n",
        "    return template.format(\n",
        "        topic=topic.title(),\n",
        "        keyword=keyword_choice[0].title(),\n",
        "        keyword2=keyword_choice[1].title(),\n",
        "        org=org,\n",
        "        person=person\n",
        "    )\n",
        "\n",
        "def choose_consistent_entities():\n",
        "    person = random.choice(list(entity_types['PERSON'])) if entity_types['PERSON'] else \"John Smith\"\n",
        "    org = random.choice(list(entity_types['ORG'])) if entity_types['ORG'] else \"Associated Press\"\n",
        "    gpe = random.choice(list(entity_types['GPE'])) if entity_types['GPE'] else \"USA\"\n",
        "    return person, org, gpe\n",
        "\n",
        "def generate_improved_article(topic_idx=None):\n",
        "    # Pilih topik utama\n",
        "    if topic_idx is None:\n",
        "        topic_idx = random.randint(0, n_topics-1)\n",
        "    topic_docs = sentences_by_topic_doc[topic_idx]\n",
        "    # Pilih dokumen dengan kalimat terbanyak\n",
        "    doc_idx = max(topic_docs, key=lambda k: len(topic_docs[k]))\n",
        "    doc_sentences = topic_docs[doc_idx]\n",
        "    # Pilih 4-5 kalimat berurutan agar koheren\n",
        "    if len(doc_sentences) > 6:\n",
        "        start = random.randint(0, len(doc_sentences)-5)\n",
        "        main_sentences = doc_sentences[start:start+5]\n",
        "    else:\n",
        "        main_sentences = doc_sentences\n",
        "\n",
        "    # Pilih entity konsisten\n",
        "    person, org, gpe = choose_consistent_entities()\n",
        "\n",
        "    # Headline\n",
        "    headline = generate_improved_headline(topic_idx, person, org)\n",
        "    article = [headline, \"\"]\n",
        "\n",
        "    # Intro\n",
        "    intro_template = random.choice(templates['intro'])\n",
        "    article.append(intro_template.format(topic=topic_names[topic_idx]))\n",
        "\n",
        "    # Paragraf utama (SEQUENTIAL SENTENCES - ADVANCED VERSION)\n",
        "    article.extend(main_sentences)\n",
        "    article.append(\"\")\n",
        "\n",
        "    # Kutipan dari kalimat yang mengandung kata 'said', 'stated', atau acak dari paragraf\n",
        "    quote_candidates = [s for s in main_sentences if re.search(r'said|stated|explained|noted', s, re.IGNORECASE)]\n",
        "    chosen_quote = random.choice(quote_candidates) if quote_candidates else random.choice(main_sentences)\n",
        "    quote_template = random.choice(templates['quote'])\n",
        "    article.append(quote_template.format(\n",
        "        sentence=chosen_quote,\n",
        "        person=person,\n",
        "        org=org,\n",
        "        verb=random.choice(verbs),\n",
        "        noun=random.choice(nouns),\n",
        "        event=random.choice(events)\n",
        "    ))\n",
        "\n",
        "    # Transisi ke topik sekunder\n",
        "    related_topics = list(range(n_topics))\n",
        "    related_topics.remove(topic_idx)\n",
        "    secondary_topic = random.choice(related_topics)\n",
        "    secondary_docs = sentences_by_topic_doc[secondary_topic]\n",
        "    if secondary_docs:\n",
        "        sec_doc_idx = max(secondary_docs, key=lambda k: len(secondary_docs[k]))\n",
        "        sec_doc_sentences = secondary_docs[sec_doc_idx]\n",
        "        if len(sec_doc_sentences) > 3:\n",
        "            start = random.randint(0, len(sec_doc_sentences)-2)\n",
        "            secondary_sentences = sec_doc_sentences[start:start+2]\n",
        "        else:\n",
        "            secondary_sentences = sec_doc_sentences\n",
        "        article.append(\"\")\n",
        "        article.append(random.choice(templates['transition']))\n",
        "        article.extend(secondary_sentences)\n",
        "\n",
        "    # Penutup\n",
        "    article.append(\"\")\n",
        "    article.append(\"Further updates will be provided as the situation develops.\")\n",
        "\n",
        "    return \"\\n\".join(article)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING ADVANCED SYNTHETIC NEWS ARTICLES (FOR RESEARCH PURPOSES ONLY)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"\\nARTICLE #{i+1}:\")\n",
        "    print(\"=\"*50)\n",
        "    article = generate_improved_article()\n",
        "    print(article)\n",
        "    print(\"=\"*50)\n",
        "\n",
        "print(\"\\nDISCLAIMER: These articles are entirely synthetic and generated for research purposes.\")\n",
        "print(\"They may contain factual inaccuracies and should not be shared as real news.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation per Function"
      ],
      "metadata": {
        "id": "4mXhbfxRP1en"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extraction to Generate"
      ],
      "metadata": {
        "id": "x8dXsf1rPOoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Extracting sentences by document...\")\n",
        "all_sentences = []\n",
        "doc_sentence_mapping = []  # Track which sentence belongs to which document\n",
        "\n",
        "for doc_idx, text in enumerate(cleaned_texts):\n",
        "    doc = nlp(text)\n",
        "    doc_sentences = []\n",
        "    for sent in doc.sents:\n",
        "        sent_text = sent.text.strip()\n",
        "        # Filter sentences that are complete and of reasonable length\n",
        "        if len(sent_text.split()) >= 5 and len(sent_text.split()) <= 30:\n",
        "            if sent_text[-1] in '.!?':\n",
        "                if not re.search(r'(^\\W|^\\d+\\.|^>|^I think|^I believe|^However|^But|^And|^Or|^So,|^Also,)', sent_text):\n",
        "                    all_sentences.append(sent_text)\n",
        "                    doc_sentences.append(sent_text)\n",
        "                    doc_sentence_mapping.append(doc_idx)"
      ],
      "metadata": {
        "id": "rCqANrWuP3QH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "919cd6a2-27df-482a-afd3-b03e21460a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting sentences by document...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vectorization & Topic Modeling"
      ],
      "metadata": {
        "id": "qemt3Oe6PSbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Extracting sentences by document...\")\n",
        "all_sentences = []\n",
        "doc_sentence_mapping = []  # Track which sentence belongs to which document\n",
        "\n",
        "for doc_idx, text in enumerate(cleaned_texts):\n",
        "    doc = nlp(text)\n",
        "    doc_sentences = []\n",
        "    for sent in doc.sents:\n",
        "        sent_text = sent.text.strip()\n",
        "        # Filter sentences that are complete and of reasonable length\n",
        "        if len(sent_text.split()) >= 5 and len(sent_text.split()) <= 30:\n",
        "            if sent_text[-1] in '.!?':\n",
        "                if not re.search(r'(^\\W|^\\d+\\.|^>|^I think|^I believe|^However|^But|^And|^Or|^So,|^Also,)', sent_text):\n",
        "                    all_sentences.append(sent_text)\n",
        "                    doc_sentences.append(sent_text)\n",
        "                    doc_sentence_mapping.append(doc_idx)"
      ],
      "metadata": {
        "id": "dFjnJA4LP8XY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "267d96d3-a9bd-498b-d3c5-843c3119a371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting sentences by document...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Grouping Sentences by Topic"
      ],
      "metadata": {
        "id": "s2sfh6EhPWDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_by_topic = defaultdict(list)\n",
        "sentences_by_topic_doc = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "for i, topic_idx in enumerate(topic_assignments):\n",
        "    sentence = all_sentences[i]\n",
        "    doc_idx = doc_sentence_mapping[i]\n",
        "\n",
        "    sentences_by_topic[topic_idx].append(sentence)\n",
        "    sentences_by_topic_doc[topic_idx][doc_idx].append(sentence)"
      ],
      "metadata": {
        "id": "T1PZF9jcQBZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extracting the main keywords from each topic"
      ],
      "metadata": {
        "id": "NtUZ_nnpPcrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract keywords for each topic\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "keywords_by_topic = {}\n",
        "\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    top_indices = topic.argsort()[:-11:-1]  # Get top 10 keywords\n",
        "    top_keywords = [feature_names[i] for i in top_indices]\n",
        "    keywords_by_topic[topic_idx] = top_keywords\n",
        "\n",
        "# Define topic names based on keywords\n",
        "topic_names = {}\n",
        "for topic_idx, keywords in keywords_by_topic.items():\n",
        "    topic_names[topic_idx] = ' '.join(keywords[:3])"
      ],
      "metadata": {
        "id": "vA9DOG9NQHec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building Entity Database"
      ],
      "metadata": {
        "id": "x7xUiLL3Pfmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create entity database for more realistic entity mentions\n",
        "entity_types = {\n",
        "    'PERSON': set(),\n",
        "    'ORG': set(),\n",
        "    'GPE': set(),  # Countries, cities\n",
        "    'DATE': set()\n",
        "}\n",
        "\n",
        "print(\"Extracting entities...\")\n",
        "# Process a subset to save time\n",
        "for text in random.sample(cleaned_texts, min(1000, len(cleaned_texts))):\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in entity_types and len(ent.text) > 1:\n",
        "            entity_types[ent.label_].add(ent.text)"
      ],
      "metadata": {
        "id": "2q-KF-2bQPt2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f9a7355-14e3-4ee1-ad42-309c0ea5db3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting entities...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Template and Helpers"
      ],
      "metadata": {
        "id": "OTzTHZ2QPkO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Template sentences for different parts of an article\n",
        "templates = {\n",
        "    'headline': [\n",
        "        \"BREAKING: {topic} - {keyword} {keyword2} Raises Concerns\",\n",
        "        \"ALERT: New Developments in {topic} as {org} Announces {keyword}\",\n",
        "        \"EXCLUSIVE: {person} Reveals Critical Information About {topic}\",\n",
        "        \"{topic} CRISIS: What {org} Isn't Telling You About {keyword}\",\n",
        "        \"JUST IN: {keyword} {keyword2} Scandal Rocks {org}\"\n",
        "    ],\n",
        "    'intro': [\n",
        "        \"A recent development regarding {topic} has sparked widespread debate.\",\n",
        "        \"New information has emerged about {topic} that demands attention.\",\n",
        "        \"In a stunning revelation, details about {topic} have come to light.\",\n",
        "        \"Reports are circulating about unprecedented events related to {topic}.\"\n",
        "    ],\n",
        "    'quote': [\n",
        "        \"\\\"{sentence}\\\" said {person}, who {verb} the {noun}.\",\n",
        "        \"\\\"{sentence}\\\" according to {person} from {org}.\",\n",
        "        \"{person} stated, \\\"{sentence}\\\" This comes after {org} {verb} the {noun}.\",\n",
        "        \"\\\"{sentence}\\\" {person} explained during the recent {event}.\"\n",
        "    ],\n",
        "    'transition': [\n",
        "        \"Meanwhile, other sources indicate that\",\n",
        "        \"This development comes at a time when\",\n",
        "        \"Experts suggest that this could mean\",\n",
        "        \"Analysis of the situation reveals\",\n",
        "        \"Critics argue that these events demonstrate\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "verbs = [\"addressed\", \"questioned\", \"investigated\", \"analyzed\", \"evaluated\",\n",
        "         \"criticized\", \"supported\", \"endorsed\", \"reviewed\", \"examined\"]\n",
        "nouns = [\"situation\", \"statement\", \"report\", \"findings\", \"allegations\",\n",
        "         \"evidence\", \"controversy\", \"development\", \"policy\", \"decision\"]\n",
        "events = [\"press conference\", \"hearing\", \"investigation\", \"meeting\", \"interview\",\n",
        "          \"briefing\", \"summit\", \"announcement\", \"statement release\"]\n",
        "\n",
        "def generate_improved_headline(topic_idx, person=None, org=None):\n",
        "    template = random.choice(templates['headline'])\n",
        "    topic = topic_names[topic_idx]\n",
        "    keywords = keywords_by_topic[topic_idx]\n",
        "    keyword_choice = random.sample(keywords, 2) if len(keywords) >= 2 else [keywords[0], keywords[0]]\n",
        "    org = org or (random.choice(list(entity_types['ORG'])) if entity_types['ORG'] else \"Organization\")\n",
        "    person = person or (random.choice(list(entity_types['PERSON'])) if entity_types['PERSON'] else \"Expert\")\n",
        "    return template.format(\n",
        "        topic=topic.title(),\n",
        "        keyword=keyword_choice[0].title(),\n",
        "        keyword2=keyword_choice[1].title(),\n",
        "        org=org,\n",
        "        person=person\n",
        "    )\n",
        "\n",
        "def choose_consistent_entities():\n",
        "    person = random.choice(list(entity_types['PERSON'])) if entity_types['PERSON'] else \"John Smith\"\n",
        "    org = random.choice(list(entity_types['ORG'])) if entity_types['ORG'] else \"Associated Press\"\n",
        "    gpe = random.choice(list(entity_types['GPE'])) if entity_types['GPE'] else \"USA\"\n",
        "    return person, org, gpe\n",
        "\n",
        "def generate_improved_article(topic_idx=None):\n",
        "    # Pilih topik utama\n",
        "    if topic_idx is None:\n",
        "        topic_idx = random.randint(0, n_topics-1)\n",
        "    topic_docs = sentences_by_topic_doc[topic_idx]\n",
        "    # Pilih dokumen dengan kalimat terbanyak\n",
        "    doc_idx = max(topic_docs, key=lambda k: len(topic_docs[k]))\n",
        "    doc_sentences = topic_docs[doc_idx]\n",
        "    # Pilih 4-5 kalimat berurutan agar koheren\n",
        "    if len(doc_sentences) > 6:\n",
        "        start = random.randint(0, len(doc_sentences)-5)\n",
        "        main_sentences = doc_sentences[start:start+5]\n",
        "    else:\n",
        "        main_sentences = doc_sentences\n",
        "\n",
        "    # Pilih entity konsisten\n",
        "    person, org, gpe = choose_consistent_entities()\n",
        "\n",
        "    # Headline\n",
        "    headline = generate_improved_headline(topic_idx, person, org)\n",
        "    article = [headline, \"\"]\n",
        "\n",
        "    # Intro\n",
        "    intro_template = random.choice(templates['intro'])\n",
        "    article.append(intro_template.format(topic=topic_names[topic_idx]))\n",
        "\n",
        "    # Paragraf utama (SEQUENTIAL SENTENCES - ADVANCED VERSION)\n",
        "    article.extend(main_sentences)\n",
        "    article.append(\"\")\n",
        "\n",
        "    # Kutipan dari kalimat yang mengandung kata 'said', 'stated', atau acak dari paragraf\n",
        "    quote_candidates = [s for s in main_sentences if re.search(r'said|stated|explained|noted', s, re.IGNORECASE)]\n",
        "    chosen_quote = random.choice(quote_candidates) if quote_candidates else random.choice(main_sentences)\n",
        "    quote_template = random.choice(templates['quote'])\n",
        "    article.append(quote_template.format(\n",
        "        sentence=chosen_quote,\n",
        "        person=person,\n",
        "        org=org,\n",
        "        verb=random.choice(verbs),\n",
        "        noun=random.choice(nouns),\n",
        "        event=random.choice(events)\n",
        "    ))\n",
        "\n",
        "    # Transisi ke topik sekunder\n",
        "    related_topics = list(range(n_topics))\n",
        "    related_topics.remove(topic_idx)\n",
        "    secondary_topic = random.choice(related_topics)\n",
        "    secondary_docs = sentences_by_topic_doc[secondary_topic]\n",
        "    if secondary_docs:\n",
        "        sec_doc_idx = max(secondary_docs, key=lambda k: len(secondary_docs[k]))\n",
        "        sec_doc_sentences = secondary_docs[sec_doc_idx]\n",
        "        if len(sec_doc_sentences) > 3:\n",
        "            start = random.randint(0, len(sec_doc_sentences)-2)\n",
        "            secondary_sentences = sec_doc_sentences[start:start+2]\n",
        "        else:\n",
        "            secondary_sentences = sec_doc_sentences\n",
        "        article.append(\"\")\n",
        "        article.append(random.choice(templates['transition']))\n",
        "        article.extend(secondary_sentences)\n",
        "\n",
        "    # Penutup\n",
        "    article.append(\"\")\n",
        "    article.append(\"Further updates will be provided as the situation develops.\")\n",
        "\n",
        "    return \"\\n\".join(article)"
      ],
      "metadata": {
        "id": "LG0hwl0MQWPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate Article"
      ],
      "metadata": {
        "id": "B7xxJk23Pnyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING ADVANCED SYNTHETIC NEWS ARTICLES (FOR RESEARCH PURPOSES ONLY)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"\\nARTICLE #{i+1}:\")\n",
        "    print(\"=\"*50)\n",
        "    article = generate_improved_article()\n",
        "    print(article)\n",
        "    print(\"=\"*50)\n",
        "\n",
        "print(\"\\nDISCLAIMER: These articles are entirely synthetic and generated for research purposes.\")\n",
        "print(\"They may contain factual inaccuracies and should not be shared as real news.\")"
      ],
      "metadata": {
        "id": "iyDdXbBuQxIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa55621d-19a5-4c39-b051-a12fc79d7400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "GENERATING ADVANCED SYNTHETIC NEWS ARTICLES (FOR RESEARCH PURPOSES ONLY)\n",
            "================================================================================\n",
            "\n",
            "ARTICLE #1:\n",
            "==================================================\n",
            "Does Help Little CRISIS: What RGB Isn't Telling You About General\n",
            "\n",
            "Reports are circulating about unprecedented events related to does help little.\n",
            "Can I get it in my house?\n",
            "Bear in mind, too, that we say here applies primarily to ordinary single-family residences.\n",
            "Where can I get a copy?\n",
            "Where can I get a copy?\n",
            "The Canadian Standards Association is an organization made up of various government agencies, power utilities, insurance companies, electrical manufacturers and other organizations.\n",
            "\n",
            "\"Can I get it in my house?\" Garry 0 explained during the recent briefing.\n",
            "\n",
            "Critics argue that these events demonstrate\n",
            "Can I install a replacement fixture?\n",
            "What is this nonsense about 3HP on 110V 15A circuits?\n",
            "\n",
            "Further updates will be provided as the situation develops.\n",
            "==================================================\n",
            "\n",
            "ARTICLE #2:\n",
            "==================================================\n",
            "BREAKING: Time Question Believe - Law Believe Raises Concerns\n",
            "\n",
            "A recent development regarding time question believe has sparked widespread debate.\n",
            "In this FAQ, \"CEC\" really means the appropriate provincial standard.\n",
            "In particular, this FAQ is derived from the Ontario Hydro Electrical Safety Code, 20th edition 1990.\n",
            "Which is in turn based on CSA C22.1-1990 16th edition.\n",
            "The appropriate provincial code can be obtained from electrical inspection offices of your provincial power authority.\n",
            "The Ontario Hydro book isn't overly fat.\n",
            "\n",
            "Migods Menschen stated, \"Which is in turn based on CSA C22.1-1990 16th edition.\" This comes after Clayton criticized the policy.\n",
            "\n",
            "Experts suggest that this could mean\n",
            "Everyone in the theater was looking at one another, Russians, Azerbaijanis, people of various nationalities.\n",
            "He stood there grief- stricken, but looking as though nothing really big had happened, like some naughty boys had just broken them quite by accident, with a slingshot.\n",
            "\n",
            "Further updates will be provided as the situation develops.\n",
            "==================================================\n",
            "\n",
            "ARTICLE #3:\n",
            "==================================================\n",
            "EXCLUSIVE: PentiumP5 Reveals Critical Information About Ve Need State\n",
            "\n",
            "Reports are circulating about unprecedented events related to ve need state.\n",
            "The orthopedic surgeon voluntarily withdrew from practice after testing positive for HIV.\n",
            "A total of 1,174 former patients underwent HIV testing, representing 50.7 percent of patients on whom the orthopedic surgeon performed invasive procedures during the 13.5-year period.\n",
            "Patients were tested from each year and from each category of invasive procedure.\n",
            "All patients were found to be negative for HIV by enzyme-linked-immunosorbent assay.\n",
            "Two former patients reported known HIV infection prior to surgery.\n",
            "\n",
            "\"All patients were found to be negative for HIV by enzyme-linked-immunosorbent assay.\" PentiumP5 explained during the recent meeting.\n",
            "\n",
            "Critics argue that these events demonstrate\n",
            "I've asked the writers to cross-post to sci.electronics in the future.\n",
            "Changes to previous issue marked with \"\" in left column.\n",
            "\n",
            "Further updates will be provided as the situation develops.\n",
            "==================================================\n",
            "\n",
            "DISCLAIMER: These articles are entirely synthetic and generated for research purposes.\n",
            "They may contain factual inaccuracies and should not be shared as real news.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "nx7xddgVzHR1",
        "aTZIi4Ic3I73",
        "L3hFxSrp426v",
        "x7Qad1ZMV9kj",
        "Z6wkwFF7LZc2",
        "7NaHC8JIRvkB",
        "LrgOa6mRZpi4",
        "wBdU7opkZ4We",
        "bRNPKPiCaCcQ",
        "B1M9s8C_aNnz",
        "5pyqj3ZPaZ7x",
        "1t1FDNBvaobf",
        "09emCO-jatqI",
        "66-ongRAa1cQ",
        "ice3Eb43a9g0",
        "lMraaSrObF9D",
        "glbMxMRKWE_j",
        "Y474nAM5dOoA",
        "781fumk8dQIl",
        "ANQo0kNfePUd",
        "NAvEvpYSeb5I",
        "LHFDVeXMed_z",
        "XPkgUqxiepcW",
        "WR0bn06ZezVj",
        "ZwZSCSxce7v3",
        "7zOOkX1KfIwx",
        "gUN9bWRYfPKD",
        "WNlacI4ffYEg",
        "zIMB_sbvfeo_",
        "GNt3Tg8CfmJZ",
        "-jBI7zSrfrTG",
        "htizXuxZ3DyO",
        "xd8pM3H8zzbM",
        "LfLiGqH7znU8",
        "W2ElUo_19SdF",
        "VMFuipci9uO9",
        "9oyGnmiEyTnJ",
        "fFvLAzyeK624",
        "4mJZU2ZS6ncw",
        "iQi7luFG-wBF",
        "WkUfRXpy8Jio",
        "YNTvrO-7Lmbp",
        "Xa1dShy2IkEH",
        "EKEz5E_LQVYc",
        "4mXhbfxRP1en",
        "x8dXsf1rPOoH",
        "qemt3Oe6PSbL",
        "s2sfh6EhPWDn",
        "NtUZ_nnpPcrB",
        "x7xUiLL3Pfmf",
        "OTzTHZ2QPkO5",
        "B7xxJk23Pnyx"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}